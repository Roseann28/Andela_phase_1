{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "sqi5B7V_Rjim"
      },
      "outputs": [],
      "source": [
        "# Copyright 2025 Google LLC\n",
        "#\n",
        "# Licensed under the Apache License, Version 2.0 (the \"License\");\n",
        "# you may not use this file except in compliance with the License.\n",
        "# You may obtain a copy of the License at\n",
        "#\n",
        "#     https://www.apache.org/licenses/LICENSE-2.0\n",
        "#\n",
        "# Unless required by applicable law or agreed to in writing, software\n",
        "# distributed under the License is distributed on an \"AS IS\" BASIS,\n",
        "# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
        "# See the License for the specific language governing permissions and\n",
        "# limitations under the License."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VyPmicX9RlZX"
      },
      "source": [
        "# Intro to Gemini 2.5 Pro\n",
        "\n",
        "\n",
        "<table align=\"left\">\n",
        "  <td style=\"text-align: center\">\n",
        "    <a href=\"https://colab.research.google.com/github/GoogleCloudPlatform/generative-ai/blob/main/gemini/getting-started/intro_gemini_2_5_pro.ipynb\">\n",
        "      <img width=\"32px\" src=\"https://www.gstatic.com/pantheon/images/bigquery/welcome_page/colab-logo.svg\" alt=\"Google Colaboratory logo\"><br> Open in Colab\n",
        "    </a>\n",
        "  </td>\n",
        "  <td style=\"text-align: center\">\n",
        "    <a href=\"https://console.cloud.google.com/vertex-ai/colab/import/https:%2F%2Fraw.githubusercontent.com%2FGoogleCloudPlatform%2Fgenerative-ai%2Fmain%2Fgemini%2Fgetting-started%2Fintro_gemini_2_5_pro.ipynb\">\n",
        "      <img width=\"32px\" src=\"https://lh3.googleusercontent.com/JmcxdQi-qOpctIvWKgPtrzZdJJK-J3sWE1RsfjZNwshCFgE_9fULcNpuXYTilIR2hjwN\" alt=\"Google Cloud Colab Enterprise logo\"><br> Open in Colab Enterprise\n",
        "    </a>\n",
        "  </td>\n",
        "  <td style=\"text-align: center\">\n",
        "    <a href=\"https://console.cloud.google.com/vertex-ai/workbench/deploy-notebook?download_url=https://raw.githubusercontent.com/GoogleCloudPlatform/generative-ai/main/gemini/getting-started/intro_gemini_2_5_pro.ipynb\">\n",
        "      <img src=\"https://www.gstatic.com/images/branding/gcpiconscolors/vertexai/v1/32px.svg\" alt=\"Vertex AI logo\"><br> Open in Vertex AI Workbench\n",
        "    </a>\n",
        "  </td>\n",
        "  <td style=\"text-align: center\">\n",
        "    <a href=\"https://github.com/GoogleCloudPlatform/generative-ai/blob/main/gemini/getting-started/intro_gemini_2_5_pro.ipynb\">\n",
        "      <img width=\"32px\" src=\"https://www.svgrepo.com/download/217753/github.svg\" alt=\"GitHub logo\"><br> View on GitHub\n",
        "    </a>\n",
        "  </td>\n",
        "</table>\n",
        "\n",
        "<div style=\"clear: both;\"></div>\n",
        "\n",
        "<b>Share to:</b>\n",
        "\n",
        "<a href=\"https://www.linkedin.com/sharing/share-offsite/?url=https%3A//github.com/GoogleCloudPlatform/generative-ai/blob/main/gemini/getting-started/intro_gemini_2_5_pro.ipynb\" target=\"_blank\">\n",
        "  <img width=\"20px\" src=\"https://upload.wikimedia.org/wikipedia/commons/8/81/LinkedIn_icon.svg\" alt=\"LinkedIn logo\">\n",
        "</a>\n",
        "\n",
        "<a href=\"https://bsky.app/intent/compose?text=https%3A//github.com/GoogleCloudPlatform/generative-ai/blob/main/gemini/getting-started/intro_gemini_2_5_pro.ipynb\" target=\"_blank\">\n",
        "  <img width=\"20px\" src=\"https://upload.wikimedia.org/wikipedia/commons/7/7a/Bluesky_Logo.svg\" alt=\"Bluesky logo\">\n",
        "</a>\n",
        "\n",
        "<a href=\"https://twitter.com/intent/tweet?url=https%3A//github.com/GoogleCloudPlatform/generative-ai/blob/main/gemini/getting-started/intro_gemini_2_5_pro.ipynb\" target=\"_blank\">\n",
        "  <img width=\"20px\" src=\"https://upload.wikimedia.org/wikipedia/commons/5/5a/X_icon_2.svg\" alt=\"X logo\">\n",
        "</a>\n",
        "\n",
        "<a href=\"https://reddit.com/submit?url=https%3A//github.com/GoogleCloudPlatform/generative-ai/blob/main/gemini/getting-started/intro_gemini_2_5_pro.ipynb\" target=\"_blank\">\n",
        "  <img width=\"20px\" src=\"https://redditinc.com/hubfs/Reddit%20Inc/Brand/Reddit_Logo.png\" alt=\"Reddit logo\">\n",
        "</a>\n",
        "\n",
        "<a href=\"https://www.facebook.com/sharer/sharer.php?u=https%3A//github.com/GoogleCloudPlatform/generative-ai/blob/main/gemini/getting-started/intro_gemini_2_5_pro.ipynb\" target=\"_blank\">\n",
        "  <img width=\"20px\" src=\"https://upload.wikimedia.org/wikipedia/commons/5/51/Facebook_f_logo_%282019%29.svg\" alt=\"Facebook logo\">\n",
        "</a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8MqT58L6Rm_q"
      },
      "source": [
        "| Authors |\n",
        "| --- |\n",
        "| [Eric Dong](https://github.com/gericdong) |\n",
        "| [Holt Skinner](https://github.com/holtskinner) |"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nVxnv1D5RoZw"
      },
      "source": [
        "## Overview\n",
        "\n",
        "**YouTube Video: Introduction to Gemini on Vertex AI**\n",
        "\n",
        "<a href=\"https://www.youtube.com/watch?v=YfiLUpNejpE&list=PLIivdWyY5sqJio2yeg1dlfILOUO2FoFRx\" target=\"_blank\">\n",
        "  <img src=\"https://img.youtube.com/vi/YfiLUpNejpE/maxresdefault.jpg\" alt=\"Introduction to Gemini on Vertex AI\" width=\"500\">\n",
        "</a>\n",
        "\n",
        "[Gemini 2.5 Pro](https://cloud.google.com/vertex-ai/generative-ai/docs/models/gemini/2-5-pro) is Google's most advanced reasoning Gemini model, to solve complex problems. With the 2.5 series, the Gemini models are now hybrid reasoning models! Gemini 2.5 Pro can apply an extended amount of thinking across tasks, and use tools in order to maximize response accuracy.\n",
        "\n",
        "Gemini 2.5 Pro is:\n",
        "\n",
        "- A significant improvement from previous models across capabilities including coding, reasoning, and multimodality\n",
        "- Industry-leading in reasoning with state of the art performance in Math & STEM benchmarks\n",
        "- An amazing model for code, with particularly strong web development\n",
        "- Particularly good for complex prompts, while still being well rounded"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WfFPCBL4Hq8x"
      },
      "source": [
        "### Objectives\n",
        "\n",
        "In this tutorial, you will learn how to use the Gemini API and the Google Gen AI SDK for Python with the Gemini 2.5 Pro model.\n",
        "\n",
        "You will complete the following tasks:\n",
        "\n",
        "- Generate text\n",
        "- Control the thinking budget\n",
        "- View summarized thoughts\n",
        "- Configure model parameters\n",
        "- Set system instructions\n",
        "- Use safety filters\n",
        "- Start a multi-turn chat\n",
        "- Use controlled generation\n",
        "- Count tokens\n",
        "- Process multimodal (audio, code, documents, images, video) data\n",
        "- Use automatic and manual function calling\n",
        "- Code execution"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gPiTOAHURvTM"
      },
      "source": [
        "## Getting Started"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CHRZUpfWSEpp"
      },
      "source": [
        "### Install Google Gen AI SDK for Python\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "sG3_LKsWSD3A",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "24a79e03-d30a-4448-cdba-053673b9352d"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m43.1/43.1 kB\u001b[0m \u001b[31m1.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m241.7/241.7 kB\u001b[0m \u001b[31m6.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h"
          ]
        }
      ],
      "source": [
        "%pip install --upgrade --quiet google-genai"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HlMVjiAWSMNX"
      },
      "source": [
        "### Authenticate your notebook environment (Colab only)\n",
        "\n",
        "If you are running this notebook on Google Colab, run the cell below to authenticate your environment."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "12fnq4V0SNV3"
      },
      "outputs": [],
      "source": [
        "import sys\n",
        "\n",
        "if \"google.colab\" in sys.modules:\n",
        "    from google.colab import auth\n",
        "\n",
        "    auth.authenticate_user()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ve4YBlDqzyj9"
      },
      "source": [
        "### Set up Google Cloud Project or API Key for Vertex AI\n",
        "\n",
        "You'll need to set up authentication by choosing **one** of the following methods:\n",
        "\n",
        "1.  **Use a Google Cloud Project:** Recommended for most users, this requires enabling the Vertex AI API in your Google Cloud project.\n",
        "    - [Enable the Vertex AI API](https://console.cloud.google.com/flows/enableapi?apiid=aiplatform.googleapis.com)\n",
        "    - Run the cell below to set your project ID and location.\n",
        "    - Read more about [Supported locations](https://cloud.google.com/vertex-ai/generative-ai/docs/learn/locations)\n",
        "2.  **Use a Vertex AI API Key (Express Mode):** For quick experimentation.\n",
        "    - [Get an API Key](https://cloud.google.com/vertex-ai/generative-ai/docs/start/express-mode/overview)\n",
        "    - See tutorial [Getting started with Gemini using Vertex AI in Express Mode](https://github.com/GoogleCloudPlatform/generative-ai/blob/main/gemini/getting-started/intro_gemini_express.ipynb).\n",
        "\n",
        "This tutorial uses a Google Cloud Project for authentication."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "a3265ecb5f26"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "\n",
        "PROJECT_ID = \"cloudshow-470509\"  # @param {type: \"string\", placeholder: \"[your-project-id]\", isTemplate: true}\n",
        "if not PROJECT_ID or PROJECT_ID == \"[your-project-id]\":\n",
        "    PROJECT_ID = str(os.environ.get(\"GOOGLE_CLOUD_PROJECT\"))\n",
        "\n",
        "LOCATION = \"global\""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EdvJRUWRNGHE"
      },
      "source": [
        "### Import libraries\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "id": "qgdSpVmDbdQ9"
      },
      "outputs": [],
      "source": [
        "from IPython.display import HTML, Image, Markdown, display\n",
        "from google import genai\n",
        "from google.genai.types import (\n",
        "    FunctionDeclaration,\n",
        "    GenerateContentConfig,\n",
        "    GoogleSearch,\n",
        "    HarmBlockThreshold,\n",
        "    HarmCategory,\n",
        "    Part,\n",
        "    SafetySetting,\n",
        "    ThinkingConfig,\n",
        "    Tool,\n",
        "    ToolCodeExecution,\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "be18ac9c5ec8"
      },
      "source": [
        "### Create a client"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "id": "3870ef96f984"
      },
      "outputs": [],
      "source": [
        "client = genai.Client(vertexai=True, project=PROJECT_ID, location=LOCATION)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "n4yRkFg6BBu4"
      },
      "source": [
        "## Use the Gemini 2.5 Pro model"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eXHJi5B6P5vd"
      },
      "source": [
        "### Load the Gemini 2.5 Pro model\n",
        "\n",
        "Learn more about all [Gemini models on Vertex AI](https://cloud.google.com/vertex-ai/generative-ai/docs/learn/models#gemini-models)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "id": "-coEslfWPrxo"
      },
      "outputs": [],
      "source": [
        "MODEL_ID = \"gemini-2.5-pro\"  # @param {type: \"string\"}"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "37CH91ddY9kG"
      },
      "source": [
        "### Generate text from text prompts\n",
        "\n",
        "Use the `generate_content()` method to generate responses to your prompts.\n",
        "\n",
        "You can pass text to `generate_content()`, and use the `.text` property to get the text content of the response.\n",
        "\n",
        "By default, Gemini outputs formatted text using [Markdown](https://daringfireball.net/projects/markdown/) syntax."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "metadata": {
        "id": "xRJuHj0KZ8xz",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "bdf8a4c5-c4bd-4054-cd36-c5617f8766d6"
      },
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Markdown object>"
            ],
            "text/markdown": "Yes, models to predict the risk of mortality, including by a specific age like 50, absolutely exist. However, it's crucial to understand what these models are, how they work, their limitations, and the significant ethical issues they raise.\n\nThey are not crystal balls. They calculate **probability and risk**, not a definite fate.\n\nHere's a breakdown of the different types of models, the data they use, and their applications.\n\n### 1. Types of Models and Their Applications\n\nThese models are primarily used in three major fields:\n\n**a) Actuarial and Insurance Models:**\nThis is the oldest and most commercially developed application. Life insurance companies have been predicting mortality for centuries to set premiums.\n\n*   **How they work:** They use statistical tables (actuarial life tables) and regression models that input a range of factors to calculate an individual's \"risk score\" compared to a baseline.\n*   **Data Used:** Age, sex, smoking status, BMI, blood pressure, cholesterol levels, family medical history, personal medical history (e.g., diabetes, heart disease), and sometimes even driving records or occupation.\n*   **Goal:** To accurately price an insurance policy. A person with higher predicted risk will pay a higher premium.\n\n**b) Public Health and Epidemiological Models:**\nResearchers use these models to understand what factors contribute to premature mortality (death before the average life expectancy) across a population.\n\n*   **How they work:** Using large datasets from population studies (like the UK Biobank or the US NHANES), researchers use statistical methods like **Survival Analysis** (e.g., Cox Proportional Hazards models) and machine learning to identify the strongest predictors of death over a specific time frame.\n*   **Data Used:** A vast range of variables, including everything insurance companies use, plus:\n    *   **Socioeconomic data:** Income, education level, zip code.\n    *   **Behavioral data:** Diet, exercise frequency, alcohol consumption, sleep patterns.\n    *   **Biomarkers:** C-reactive protein (inflammation), kidney function, liver enzymes.\n    *   **Physical function:** Grip strength, walking speed. (Slow walking speed has been shown to be a surprisingly strong predictor of mortality).\n*   **Goal:** To identify at-risk groups and modifiable risk factors (like diet and exercise) to inform public health policy and interventions.\n\n**c) Clinical and Biomedical Models:**\nThese are used in a medical setting to help doctors assess a patient's risk and make treatment decisions.\n\n*   **How they work:** These are often specific \"risk scores\" for patients with a particular condition. For example, the Framingham Risk Score predicts the 10-year risk of a cardiovascular event. A high score might lead a doctor to prescribe statins or blood pressure medication more aggressively. Recently, AI models are being developed to predict outcomes from electronic health records (EHRs) or medical imaging.\n*   **Data Used:** Highly specific clinical data, such as lab results, vital signs, genetic markers, diagnoses from EHRs, and details from X-rays or CT scans.\n*   **Goal:** To stratify patient risk, personalize treatment, and improve clinical outcomes.\n\n### Key Predictors of Mortality by 50\n\nPredicting death by 50 is essentially predicting **premature mortality**. The most powerful predictors are:\n\n1.  **Genetics:** Certain genetic conditions (e.g., Huntington's disease, familial hypercholesterolemia) and specific gene variants (like APOE4 for Alzheimer's) significantly increase risk. **Polygenic Risk Scores**, which aggregate the effects of thousands of small-effect genes, are becoming more powerful.\n2.  **Lifestyle & Behavior:** This is the most significant *modifiable* category.\n    *   **Smoking** is one of the single strongest predictors of early death.\n    *   **Heavy alcohol and drug use.**\n    *   **Obesity** (high BMI).\n    *   **Sedentary lifestyle** and poor physical fitness.\n3.  **Chronic Disease:** The presence of diseases like type 2 diabetes, heart disease, kidney disease, or cancer diagnosed at a young age is a major predictor.\n4.  **Socioeconomic Status:** Lower income, lower education levels, and living in a deprived area are all strongly correlated with higher rates of premature mortality due to a combination of factors including access to healthcare, environmental exposures, chronic stress, and nutrition.\n5.  **Mental Health:** Severe, untreated depression and other mental illnesses are linked to higher mortality rates, both through suicide and through their impact on physical health.\n\n### Accuracy and Limitations\n\n*   **Probabilistic, Not Deterministic:** It's the most important thing to remember. A model might say you have a 15% risk of dying by 50, not that you *will* die. This means that out of 100 people with your exact risk profile, 15 are predicted to die. It says nothing definitive about *you*.\n*   **Population vs. Individual:** These models are far more accurate for predicting outcomes for a *group* of people than for a single individual.\n*   **Data Quality and Bias:** The models are only as good as the data they are trained on. If the data is biased against certain demographic groups, the model's predictions will also be biased.\n*   **Risk is Dynamic:** A model's prediction is a snapshot in time. A person can dramatically change their risk by quitting smoking, starting to exercise, or getting a disease under control.\n\n### Ethical Implications\n\nThis is where the topic gets very sensitive.\n\n*   **Discrimination:** Could this information be used by employers to not hire someone, or by banks to deny a loan? In the US, the Genetic Information Nondiscrimination Act (GINA) provides some protection, but loopholes exist, and it doesn't cover life insurance.\n*   **Psychological Harm:** Being told you have a high risk of early death could cause immense anxiety, depression, and fatalism, potentially leading to a self-fulfilling prophecy where a person stops trying to be healthy.\n*   **Data Privacy:** This is arguably the most sensitive personal data that can exist. Who has the right to create, store, and access this information?\n*   **Inequity:** If these models are used to allocate healthcare resources, they could end up diverting resources away from the very populations who are already disadvantaged and have the highest risk.\n\n**Conclusion:**\n\nYes, models to predict whether an individual will be dead by 50 exist and are used in research, insurance, and medicine. They work by analyzing statistical relationships between a vast array of data points—from genetics to lifestyle—and mortality.\n\nHowever, they provide a **risk probability, not a certainty**, and are fraught with major accuracy limitations and profound ethical challenges that society is still grappling with."
          },
          "metadata": {}
        }
      ],
      "source": [
        "response = client.models.generate_content(\n",
        "    model=MODEL_ID, contents=\"Are there models to predict whether an individual will be dead by 50?\"\n",
        ")\n",
        "\n",
        "display(Markdown(response.text))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JkYQATRxAK1_"
      },
      "source": [
        "#### Example prompts\n",
        "\n",
        "- What are the biggest challenges facing the healthcare industry?\n",
        "- What are the latest developments in the automotive industry?\n",
        "- What are the biggest opportunities in retail industry?\n",
        "- (Try your own prompts!)\n",
        "\n",
        "For more examples of prompt engineering, refer to [this notebook](https://github.com/GoogleCloudPlatform/generative-ai/blob/main/gemini/prompts/intro_prompt_design.ipynb)."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "95aeab702af3"
      },
      "source": [
        "### Control the thinking budget\n",
        "\n",
        "You set the optional `thinking_budget` parameter in the `ThinkingConfig` to control and configure how much a model thinks on a given user prompt. The `thinking_budget` sets the upper limit on the number of tokens to use for reasoning for certain tasks. It allows users to control quality and speed of response.\n",
        "\n",
        "**Notes**\n",
        "\n",
        "- By default, the model automatically controls how much it thinks up to a maximum of 8192 tokens.\n",
        "- The maximum thinking budget that you can set is `32768` tokens, and the minimum you can set is `128`.\n",
        "\n",
        "Then use the `generate_content` or `generate_content_stream` method to send a request to generate content with the `thinking_config`."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {
        "id": "364133e30ba5",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 46
        },
        "outputId": "58a833fd-6309-4889-ee56-dbdf9cf93e4e"
      },
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Markdown object>"
            ],
            "text/markdown": "There are **three** R's in the word \"strawberry\"."
          },
          "metadata": {}
        }
      ],
      "source": [
        "THINKING_BUDGET = 1024  # @param {type: \"integer\"}\n",
        "\n",
        "response = client.models.generate_content(\n",
        "    model=MODEL_ID,\n",
        "    contents=\"How many R's are in the word strawberry?\",\n",
        "    config=GenerateContentConfig(\n",
        "        thinking_config=ThinkingConfig(\n",
        "            thinking_budget=THINKING_BUDGET,\n",
        "        )\n",
        "    ),\n",
        ")\n",
        "\n",
        "display(Markdown(response.text))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "05dc39e0c6b5"
      },
      "source": [
        "Optionally, you can print the usage_metadata and token counts from the model response."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {
        "id": "7981c3442177",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "9bfa2db6-a156-4979-c933-f8568f939906"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "prompt_token_count: 11\n",
            "candidates_token_count: 14\n",
            "thoughts_token_count: 242\n",
            "total_token_count: 267\n"
          ]
        }
      ],
      "source": [
        "print(f\"prompt_token_count: {response.usage_metadata.prompt_token_count}\")\n",
        "print(f\"candidates_token_count: {response.usage_metadata.candidates_token_count}\")\n",
        "print(f\"thoughts_token_count: {response.usage_metadata.thoughts_token_count}\")\n",
        "print(f\"total_token_count: {response.usage_metadata.total_token_count}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "c66712160c15"
      },
      "source": [
        "### View summarized thoughts\n",
        "\n",
        "You can optionally set the `include_thoughts` flag to enable the model to generate and return a summary of the \"thoughts\" that it generates in addition to the final answer.\n",
        "\n",
        "In this example, you use the `generate_content` method to send a request to generate content with summarized thoughts. The model responds with multiple parts, the thoughts and the model response. You can check the `part.thought` field to determine if a part is a thought or not."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {
        "id": "60d74a351671",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 330
        },
        "outputId": "fcdd68dc-5151-43bb-9018-32add6f3cc5b"
      },
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Markdown object>"
            ],
            "text/markdown": "## Thoughts:\n         Okay, here's what I'm thinking. The user wants to know how many \"R\"s are in the word \"strawberry.\" Alright, easy enough. First things first, I'll state the word, just to solidify the starting point: \"strawberry.\" Now, I need to systematically scan the word, letter by letter, and look for the letter \"R.\"\n\nLet's go through it: \"s\" - no, \"t\" - no, \"r\" - ah, one! \"a\" - no, \"w\" - no, \"b\" - no, \"e\" - no, \"r\" - two! \"r\" - three! \"y\" - no. Got it.\n\nSo, I've found three \"R\"s. Time to construct the answer. I'll start directly: \"There are three R's in the word strawberry.\" Just to be extra clear and offer a visual aid, I'll emphasize the \"R\"s within the word itself: \"st**r**awbe**rr**y.\" The user asked a straightforward question, so I’ll keep the response simple and direct.\n\nWait... (internal check) The user used an uppercase \"R,\" but the word \"strawberry\" is all lowercase. Does case matter? Not usually. Unless the user explicitly specifies case sensitivity (which they haven't), the intent is clear: count the letter, regardless of case. Since the word only contains lowercase 'r's, counting those is the correct approach.\n\nOkay, final answer: \"There are three R's in the word strawberry.\" Count confirmed, answer clear.\n\n        "
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Markdown object>"
            ],
            "text/markdown": "## Answer:\n         There are **three** R's in the word st**r**awbe**rr**y.\n        "
          },
          "metadata": {}
        }
      ],
      "source": [
        "response = client.models.generate_content(\n",
        "    model=MODEL_ID,\n",
        "    contents=\"How many R's are in the word strawberry?\",\n",
        "    config=GenerateContentConfig(\n",
        "        thinking_config=ThinkingConfig(\n",
        "            include_thoughts=True,\n",
        "        )\n",
        "    ),\n",
        ")\n",
        "\n",
        "for part in response.candidates[0].content.parts:\n",
        "    if part.thought:\n",
        "        display(\n",
        "            Markdown(\n",
        "                f\"\"\"## Thoughts:\n",
        "         {part.text}\n",
        "        \"\"\"\n",
        "            )\n",
        "        )\n",
        "    else:\n",
        "        display(\n",
        "            Markdown(\n",
        "                f\"\"\"## Answer:\n",
        "         {part.text}\n",
        "        \"\"\"\n",
        "            )\n",
        "        )"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6lLIxqS6_-l8"
      },
      "source": [
        "### Generate content stream\n",
        "\n",
        "By default, the model returns a response after completing the entire generation process. You can also use the `generate_content_stream` method to stream the response as it is being generated, and the model will return chunks of the response as soon as they are generated.\n",
        "\n",
        "This example shows how to set the `include_thoughts` and `thinking_budget` in the `generate_content_stream` method."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 21,
      "metadata": {
        "id": "ZiwWBhXsAMnv",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 500
        },
        "outputId": "09131655-93f0-4abd-9736-44bdfa21d911"
      },
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Markdown object>"
            ],
            "text/markdown": "## Thoughts"
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Markdown object>"
            ],
            "text/markdown": "**Understanding the Problem**\n\nI've got the user's question nailed down: figuring out the ball's price in that classic riddle. Recognizing the riddle itself, I can see the bait-and-switch setup, where an easy answer ($0) is the trap.\n\n\n"
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Markdown object>"
            ],
            "text/markdown": "**Calculating the Solution**\n\nI'm now working through the math.  I've clearly defined the variables, set up the equations, and now I'm using substitution to solve for 'L', the ball's cost. A quick simplification and I've got it! The ball costs $0.05.\n\n\n"
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Markdown object>"
            ],
            "text/markdown": "**Crafting a Concise Response**\n\nOkay, I'm now focused on composing the perfect response for the user. I'm aiming for a clear, concise, and easily digestible explanation, complete with the correct answer and the logic. I've mapped out the steps, avoiding jargon, and plan to address the common mistake.\n\n\n"
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Markdown object>"
            ],
            "text/markdown": "## Answer"
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Markdown object>"
            ],
            "text/markdown": "This is a classic brain teaser! Here's the breakdown:\n\nThe ball costs **5 cents** ($0.05).\n\nHere's why:\n\n*   The ball costs **$0.05**\n*   The bat costs $1.00 more, which makes it **$1"
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Markdown object>"
            ],
            "text/markdown": ".05**\n*   The total cost is $1.05 + $0.05 = **$1.10**\n\nThe common (but incorrect) answer is 10 cents. If the ball were 10 cents, the bat would have to be $1.10"
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Markdown object>"
            ],
            "text/markdown": " ($1.00 more), and the total would be $1.20."
          },
          "metadata": {}
        }
      ],
      "source": [
        "THINKING_BUDGET = 1024  # @param {type: \"integer\"}\n",
        "INCLUDE_THOUGHTS = True  # @param {type: \"boolean\"}\n",
        "\n",
        "prompt = \"\"\"\n",
        "A bat and a ball cost $1.10 in total.\n",
        "The bat costs $1.00 more than the ball.\n",
        "How much does the ball cost?\n",
        "\"\"\"\n",
        "\n",
        "thoughts = \"\"\n",
        "answer = \"\"\n",
        "\n",
        "for chunk in client.models.generate_content_stream(\n",
        "    model=MODEL_ID,\n",
        "    contents=prompt,\n",
        "    config=GenerateContentConfig(\n",
        "        thinking_config=ThinkingConfig(\n",
        "            thinking_budget=THINKING_BUDGET,\n",
        "            include_thoughts=INCLUDE_THOUGHTS,\n",
        "        )\n",
        "    ),\n",
        "):\n",
        "\n",
        "    for part in chunk.candidates[0].content.parts:\n",
        "        if not part.text:\n",
        "            continue\n",
        "        elif part.thought:\n",
        "            if not thoughts:\n",
        "                display(Markdown(\"## Thoughts\"))\n",
        "            display(Markdown(part.text))\n",
        "            thoughts += part.text\n",
        "        else:\n",
        "            if not answer:\n",
        "                display(Markdown(\"## Answer\"))\n",
        "            display(Markdown(part.text))\n",
        "            answer += part.text"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "df5a184feb04"
      },
      "source": [
        "## Thinking examples\n",
        "\n",
        "The following examples are some complex tasks that require multiple rounds of strategizing and iteratively solving.\n",
        "\n",
        "### **Thinking example 1**: Code generation\n",
        "\n",
        "Gemini 2.5 Pro excels at creating visually compelling web apps and agentic code applications, along with code transformation and editing.\n",
        "\n",
        "Let's see how the model uses its reasoning capabilities to create a video game, using executable code from a single line prompt. See the example game [here](https://www.youtube.com/watch?v=RLCBSpgos6s)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 22,
      "metadata": {
        "id": "598bafe38bba",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "98a5bbc1-f22f-45c9-fa90-29791a99388b"
      },
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Markdown object>"
            ],
            "text/markdown": "Of course! Here is a complete, self-contained p5.js sketch for a captivating endless runner game featuring a pixelated dinosaur and a multi-layered parallax background.\n\nJust copy and paste this entire code block into the [p5.js Web Editor](https://editor.p5js.org/) and press play.\n\n### Key Features:\n*   **Pixel Art:** All graphics are drawn directly in the code, no external files needed.\n*   **Animated Dino:** The dinosaur has a simple 2-frame run animation.\n*   **Parallax Background:** Three layers of background (distant mountains, closer hills, and ground details) scroll at different speeds to create a sense of depth.\n*   **Multiple Obstacles:** The dino must dodge both ground-based cacti and flying Pterodactyls.\n*   **Increasing Difficulty:** The game speeds up and obstacles spawn more frequently over time.\n*   **On-Screen Instructions:** All text for starting, playing, and restarting is displayed directly on the canvas.\n\n```javascript\n// --- GAME CONFIGURATION ---\nlet gameFont;\nlet dino;\nlet obstacles = [];\nlet backgroundLayers = [];\nlet score = 0;\nlet gameSpeed = 6;\nlet initialGameSpeed = 6;\nlet maxGameSpeed = 15;\nlet spawnTimer = 0;\nlet initialSpawnRate = 100; // Lower is faster\nlet spawnRate = initialSpawnRate;\n\nlet gameState = 'start'; // 'start', 'playing', 'gameOver'\n\nconst groundHeight = 50;\nconst gravity = 0.7;\nconst jumpForce = -17;\nconst pixelSize = 4;\n\n// --- PIXEL ART DEFINITIONS ---\n// 'c' = main color, 'd' = dark shade, ' ' = empty\nconst dinoArtRun1 = [\n  \"       cc\",\n  \"      cdc\",\n  \"      ccc\",\n  \" ccc ccc \",\n  \"cc ccc c \",\n  \" cccccc  \",\n  \"  ccc    \",\n  \"  c c    \",\n  \"  c c    \",\n];\n\nconst dinoArtRun2 = [\n  \"       cc\",\n  \"      cdc\",\n  \"      ccc\",\n  \" ccc ccc \",\n  \"cc ccc c \",\n  \" cccccc  \",\n  \"  ccc    \",\n  \"   c c   \",\n  \"   c c   \",\n];\n\n// 'g' = green, 'd' = dark green\nconst cactusArt1 = [\n  \" g \",\n  \"ggg\",\n  \" g \",\n  \" g \",\n  \" g \",\n];\n\nconst cactusArt2 = [\n  \" g \",\n  \"ggd\",\n  \" g \",\n  \"gdg\",\n  \" g \",\n];\n\nconst pterodactylArt = [\n    \"   ccccc\",\n    \" ccdcccc\",\n    \"ccddccc \",\n    \" dddc   \",\n    \"  c     \",\n];\n\n\n// --- P5.JS SETUP FUNCTION ---\nfunction setup() {\n  createCanvas(800, 450);\n  noSmooth(); // Essential for a crisp pixelated look\n\n  // Create background layers\n  // Layer: { color, speedMultiplier, elements: [] }\n  backgroundLayers.push(createLayer('#4a4a69', 0.2, 10, 30, 100)); // Distant mountains\n  backgroundLayers.push(createLayer('#6b6b94', 0.4, 15, 20, 50));  // Closer hills\n  backgroundLayers.push(createLayer('#2a532e', 1, 40, 5, 10));     // Ground details\n\n  resetGame();\n}\n\n// --- P5.JS DRAW FUNCTION (MAIN GAME LOOP) ---\nfunction draw() {\n  background('#87CEEB'); // Sky blue\n\n  // Draw background\n  drawBackground();\n  drawGround();\n\n  // Game State Machine\n  switch (gameState) {\n    case 'start':\n      drawStartScreen();\n      break;\n    case 'playing':\n      runGame();\n      break;\n    case 'gameOver':\n      drawGameOverScreen();\n      break;\n  }\n}\n\n// --- GAME STATE FUNCTIONS ---\nfunction runGame() {\n  // Update and draw dino\n  dino.update();\n  dino.show();\n\n  // Spawn obstacles\n  handleObstacles();\n\n  // Update Score and Difficulty\n  score++;\n  gameSpeed = min(maxGameSpeed, initialGameSpeed + score / 200);\n  spawnRate = max(40, initialSpawnRate - score / 100);\n\n  // Display Score\n  drawText(`SCORE: ${score}`, 20, 20, 24, LEFT);\n}\n\nfunction drawStartScreen() {\n  dino.show(); // Show the idle dino\n  drawText(\"PIXEL DINO RUN\", width / 2, height / 3, 64, CENTER);\n  drawText(\"Press SPACE to Start\", width / 2, height / 2, 32, CENTER);\n  drawText(\"Use SPACE to Jump\", width / 2, height / 2 + 50, 24, CENTER);\n}\n\nfunction drawGameOverScreen() {\n  // Show final state\n  dino.show();\n  obstacles.forEach(obs => obs.show());\n\n  drawText(\"GAME OVER\", width / 2, height / 3, 64, CENTER);\n  drawText(`Final Score: ${score}`, width / 2, height / 2, 32, CENTER);\n  drawText(\"Press 'R' to Restart\", width / 2, height / 2 + 50, 24, CENTER);\n}\n\nfunction resetGame() {\n  dino = new Dino();\n  obstacles = [];\n  score = 0;\n  gameSpeed = initialGameSpeed;\n  spawnRate = initialSpawnRate;\n  spawnTimer = 0;\n  gameState = 'start';\n}\n\n// --- INPUT HANDLING ---\nfunction keyPressed() {\n  if (key === ' ') {\n    if (gameState === 'playing') {\n      dino.jump();\n    } else if (gameState === 'start') {\n      gameState = 'playing';\n    }\n  }\n  if (key === 'r' || key === 'R') {\n    if (gameState === 'gameOver') {\n      resetGame();\n    }\n  }\n}\n\n// --- HELPER & DRAWING FUNCTIONS ---\n\nfunction drawText(str, x, y, size, align) {\n  push();\n  textAlign(align, TOP);\n  textSize(size);\n  fill(50, 50, 50, 150);\n  text(str, x + 2, y + 2); // Shadow\n  fill(255);\n  text(str, x, y);\n  pop();\n}\n\nfunction drawPixelArt(art, x, y, pSize, colorMap) {\n  for (let i = 0; i < art.length; i++) {\n    for (let j = 0; j < art[i].length; j++) {\n      const char = art[i][j];\n      if (colorMap[char]) {\n        push();\n        noStroke();\n        fill(colorMap[char]);\n        rect(x + j * pSize, y + i * pSize, pSize, pSize);\n        pop();\n      }\n    }\n  }\n}\n\nfunction drawGround() {\n  push();\n  fill('#4a2e2b'); // Dark brown earth\n  rect(0, height - groundHeight, width, groundHeight);\n  fill('#528037'); // Green grass top\n  rect(0, height - groundHeight, width, 10);\n  pop();\n}\n\nfunction drawBackground() {\n  for (const layer of backgroundLayers) {\n    push();\n    fill(layer.color);\n    noStroke();\n    for (const el of layer.elements) {\n      // Draw the background element (e.g., a simple triangle for a mountain)\n      triangle(el.x, height - groundHeight,\n               el.x + el.w / 2, height - groundHeight - el.h,\n               el.x + el.w, height - groundHeight);\n\n      // Move the element\n      if (gameState === 'playing') {\n        el.x -= gameSpeed * layer.speedMultiplier;\n      }\n\n      // If it moves off-screen, wrap it around to the right\n      if (el.x + el.w < 0) {\n        el.x = width + random(20, 100);\n      }\n    }\n    pop();\n  }\n}\n\n\nfunction handleObstacles() {\n  // Spawn new obstacles\n  spawnTimer++;\n  if (spawnTimer > spawnRate) {\n    spawnTimer = 0;\n    obstacles.push(new Obstacle());\n  }\n\n  // Update and draw existing obstacles\n  for (let i = obstacles.length - 1; i >= 0; i--) {\n    obstacles[i].update();\n    obstacles[i].show();\n\n    // Check for collision\n    if (dino.hits(obstacles[i])) {\n      gameState = 'gameOver';\n    }\n\n    // Remove obstacles that are off-screen\n    if (obstacles[i].isOffscreen()) {\n      obstacles.splice(i, 1);\n    }\n  }\n}\n\nfunction createLayer(color, speed, count, minH, maxH) {\n    const layer = {\n        color: color,\n        speedMultiplier: speed,\n        elements: []\n    };\n    for (let i = 0; i < count; i++) {\n        layer.elements.push({\n            x: random(-width, width * 2), // Spread them out initially\n            h: random(minH, maxH),\n            w: random(50, 150)\n        });\n    }\n    return layer;\n}\n\n// --- CLASSES ---\n\nclass Dino {\n  constructor() {\n    this.w = dinoArtRun1[0].length * pixelSize;\n    this.h = dinoArtRun1.length * pixelSize;\n    this.x = 60;\n    this.y = height - groundHeight - this.h;\n    this.vy = 0; // Velocity in y-direction\n    this.baseY = this.y;\n    this.onGround = true;\n\n    this.runFrame = 0;\n    this.colorMap = { c: '#3e8043', d: '#2a532e' }; // Green dino\n  }\n\n  jump() {\n    if (this.onGround) {\n      this.vy = jumpForce;\n      this.onGround = false;\n    }\n  }\n\n  update() {\n    this.vy += gravity;\n    this.y += this.vy;\n\n    // Prevent falling through the ground\n    if (this.y >= this.baseY) {\n      this.y = this.baseY;\n      this.vy = 0;\n      this.onGround = true;\n    }\n\n    // Update animation frame\n    if (this.onGround) {\n        this.runFrame = floor(frameCount / 6) % 2;\n    }\n  }\n\n  hits(obstacle) {\n    // Simple Axis-Aligned Bounding Box collision detection\n    return (\n      this.x < obstacle.x + obstacle.w &&\n      this.x + this.w > obstacle.x &&\n      this.y < obstacle.y + obstacle.h &&\n      this.y + this.h > obstacle.y\n    );\n  }\n\n  show() {\n    const currentArt = this.runFrame === 0 ? dinoArtRun1 : dinoArtRun2;\n    drawPixelArt(currentArt, this.x, this.y, pixelSize, this.colorMap);\n  }\n}\n\n\nclass Obstacle {\n  constructor() {\n    this.x = width; // Start off-screen to the right\n    this.type = random() > 0.3 ? 'cactus' : 'pterodactyl';\n\n    if (this.type === 'cactus') {\n        this.art = random() > 0.5 ? cactusArt1 : cactusArt2;\n        this.w = this.art[0].length * pixelSize;\n        this.h = this.art.length * pixelSize;\n        this.y = height - groundHeight - this.h;\n        this.colorMap = { g: '#6b8e23', d: '#556b2f' }; // Olive green\n    } else { // Pterodactyl\n        this.art = pterodactylArt;\n        this.w = this.art[0].length * pixelSize;\n        this.h = this.art.length * pixelSize;\n        // Fly at one of two heights\n        this.y = height - groundHeight - this.h - (random() > 0.5 ? 40 : 80);\n        this.colorMap = { c: '#d2691e', d: '#8b4513' }; // Chocolate/brown\n    }\n  }\n\n  update() {\n    this.x -= gameSpeed;\n  }\n\n  isOffscreen() {\n    return this.x + this.w < 0;\n  }\n\n  show() {\n    drawPixelArt(this.art, this.x, this.y, pixelSize, this.colorMap);\n  }\n}\n```"
          },
          "metadata": {}
        }
      ],
      "source": [
        "prompt = \"\"\"\n",
        "  Make me a captivating endless runner game. Key instructions on the screen. p5js scene, no HTML.\n",
        "  I like pixelated dinosaurs and interesting backgrounds.\n",
        "\"\"\"\n",
        "\n",
        "response = client.models.generate_content(\n",
        "    model=MODEL_ID,\n",
        "    contents=prompt,\n",
        "    config=GenerateContentConfig(\n",
        "        thinking_config=ThinkingConfig(\n",
        "            thinking_budget=8196,\n",
        "        )\n",
        "    ),\n",
        ")\n",
        "\n",
        "display(Markdown(response.text))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9f00955d5b33"
      },
      "source": [
        "### **Thinking example 2**: Multimodal reasoning (Geometry)\n",
        "\n",
        "This geometry problem requires complex reasoning and is also using multimodal capabilities to reason across text and image."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 23,
      "metadata": {
        "id": "1b9975dcd0f0",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 409
        },
        "outputId": "0a777cfc-95e4-4eb5-b2aa-31d00f839ac9"
      },
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "<img src=\"https://storage.googleapis.com/generativeai-downloads/images/geometry.png\" width=\"400\"/>"
            ],
            "text/plain": [
              "<IPython.core.display.Image object>"
            ]
          },
          "metadata": {}
        }
      ],
      "source": [
        "image_file_url = (\n",
        "    \"https://storage.googleapis.com/generativeai-downloads/images/geometry.png\"\n",
        ")\n",
        "display(Image(url=image_file_url, width=400))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 24,
      "metadata": {
        "id": "43843bf748f1",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 738
        },
        "outputId": "f5111130-7b94-4877-a91c-189781d3c470"
      },
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Markdown object>"
            ],
            "text/markdown": "## Thoughts:\n         Alright, let's break down this geometry problem. I see a circle and a right triangle overlapping, and those pesky \"3\"s are everywhere. My first step is always to dissect the given: I have an image with a circle, a right triangle, and some hints in the form of \"3\" labels. The goal is clear: determine the area of the overlapping region. I immediately start visualizing, and realize the shape is a quarter-circle.\n\nOkay, so I'm looking at shapes and their properties. The blue circle has radii labeled \"3,\" which is a huge hint. I've got a circle with a radius of 3. Next, the green triangle. Definitely a right triangle, with that right angle right where I'd expect to find it. And a stroke of luck, the right-angle vertex coincides with the circle's center. Crucial observation! Analyzing the sides, the \"3\" seems to mark out the radius on the sides of the triangle, meaning that the circle neatly slices off a corner.\n\nNow, I'm visualizing the overlap. Since the circle's center *is* the triangle's right-angle vertex, the overlapping area is a sector. And given the right angle, it's a sector that's exactly a quarter of the circle – a perfect \"pizza slice.\"\n\nSo, calculation time. I've got a radius (r = 3) and a sector angle of 90 degrees (or π/2 radians – but I'm thinking degrees because that 90° is easy to grasp). The formula for the area of a sector is `(θ/360) * π * r²`, or in radians, `(1/2) * θ * r²`. Since the degree calculation is easier here, I’ll go with that one.\n\nPlugging in the numbers: `A_overlap = (90 / 360) * π * 3²`. Simple arithmetic:  `A_overlap = (1/4) * π * 9 = 9π / 4`.  Or, if I want a more intuitive feel, roughly 7.07.\n\nI pause, I mentally check. Does it make sense? Yep. It's a quarter of a circle, so it's a fraction of πr². It's good. All that extra information about the lengths of the triangle’s sides – all the 6's – are just there for context. What I needed was that overlapping quarter circle.\n\nTherefore, the area of the overlapping region is exactly 9π / 4. It’s a quarter of the full circle's area, as expected.\n\n        "
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Markdown object>"
            ],
            "text/markdown": "## Answer:\n         Based on the image, we can determine the area of the overlapping region by following these steps:\n\n1.  **Identify the shapes and their properties:**\n    *   The blue shape is a circle. The lines drawn from the center to the edge are labeled with the number \"3\". This indicates that the **radius (r) of the circle is 3**.\n    *   The green shape is a right-angled triangle.\n    *   Crucially, the **center of the circle is located at the vertex of the right angle** of the triangle.\n\n2.  **Describe the overlapping region:**\n    *   The overlapping region is the part of the circle that lies inside the triangle.\n    *   Since the circle's center is at the triangle's 90° corner, the overlap forms a **sector of the circle**.\n    *   The angle of this sector is the same as the corner of the triangle, which is **90 degrees**.\n\n3.  **Calculate the area:**\n    *   A 90-degree sector is exactly **one-quarter (1/4)** of a full circle.\n    *   The formula for the area of a full circle is `Area = π * r²`.\n    *   First, let's find the area of the entire circle with a radius of 3:\n        Area_full_circle = π * (3)² = **9π**\n    *   Now, to find the area of the overlapping quarter-circle, we divide the total area by 4:\n        Area_overlap = (1/4) * Area_full_circle = **9π / 4**\n\nThe area of the overlapping region is **9π/4**.\n        "
          },
          "metadata": {}
        }
      ],
      "source": [
        "response = client.models.generate_content(\n",
        "    model=MODEL_ID,\n",
        "    contents=[\n",
        "        Part.from_uri(file_uri=image_file_url, mime_type=\"image/png\"),\n",
        "        \"What's the area of the overlapping region?\",\n",
        "    ],\n",
        "    config=GenerateContentConfig(\n",
        "        thinking_config=ThinkingConfig(\n",
        "            include_thoughts=True,\n",
        "        )\n",
        "    ),\n",
        ")\n",
        "\n",
        "for part in response.candidates[0].content.parts:\n",
        "    if part.thought:\n",
        "        display(\n",
        "            Markdown(\n",
        "                f\"\"\"## Thoughts:\n",
        "         {part.text}\n",
        "        \"\"\"\n",
        "            )\n",
        "        )\n",
        "    else:\n",
        "        display(\n",
        "            Markdown(\n",
        "                f\"\"\"## Answer:\n",
        "         {part.text}\n",
        "        \"\"\"\n",
        "            )\n",
        "        )"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ddf356ac9cce"
      },
      "source": [
        "### **Thinking example 3**:  Math and problem solving\n",
        "\n",
        "Here's another brain teaser based on an image, this time it looks like a mathematical problem, but it cannot actually be solved mathematically. If you check the thoughts of the model you'll see that it will realize it and come up with an out-of-the-box solution."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 25,
      "metadata": {
        "id": "447f05072790",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 348
        },
        "outputId": "cadd96dd-99e1-4661-a8f1-3c0a99e184c7"
      },
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "<img src=\"https://storage.googleapis.com/generativeai-downloads/images/pool.png\" width=\"400\"/>"
            ],
            "text/plain": [
              "<IPython.core.display.Image object>"
            ]
          },
          "metadata": {}
        }
      ],
      "source": [
        "image_file_url = \"https://storage.googleapis.com/generativeai-downloads/images/pool.png\"\n",
        "display(Image(url=image_file_url, width=400))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 26,
      "metadata": {
        "id": "dc1faf95ce6f",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 423
        },
        "outputId": "7cfc68a3-637d-460b-9157-0d6cbe7be778"
      },
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Markdown object>"
            ],
            "text/markdown": "## Thoughts:\n         Alright, let's see... the user wants me to figure out how to get a sum of 30 using three of those pool balls in the image. Simple enough, or so it seems. First things first: the balls are numbered 7, 9, 11, and 13. And the goal is a sum of 30. Okay, so that translates directly into a straightforward mathematical problem: find three numbers from that set that add up to 30.\n\nNow, let's think. All those numbers are odd. Ah, that triggers a quick mental calculation. We all know that odd + odd = even, and then even + odd = odd. Which means that the sum of any three odd numbers has *got* to be odd. But 30 is... even. Hmmm. Well, that's a dead end. Directly adding those numbers just won't work. This is probably not a straightforward calculation, but a riddle.\n\nOkay, what's the angle here? What kind of trickery can we employ? A visual trick perhaps? Maybe we can rotate one of the balls? The most likely candidate is the 9. Could it be turned into something else? Aha! Flip that 9 upside down and it becomes a 6. That changes the available numbers to {7, 6, 11, 13}. Now, can we get 30 with these? Sure, 6 + 11 + 13 = 30! That's it!\n\nSo, the answer is: Mathematically speaking, you can't get a sum of 30 by adding three of the numbers 7, 9, 11, and 13. However, this is a classic riddle! The trick is to flip the 9 ball upside down, making it a 6. Then, 6 + 11 + 13 = 30. Done.\n\n        "
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Markdown object>"
            ],
            "text/markdown": "## Answer:\n         This is a classic riddle!\n\nHere's how you do it:\n\nYou turn the **9** ball upside down to make it a **6**.\n\nThen, you can add the three balls:\n\n**6 + 11 + 13 = 30**\n        "
          },
          "metadata": {}
        }
      ],
      "source": [
        "response = client.models.generate_content(\n",
        "    model=MODEL_ID,\n",
        "    contents=[\n",
        "        Part.from_uri(file_uri=image_file_url, mime_type=\"image/png\"),\n",
        "        \"How do I use three of the pool balls to sum up to 30?\",\n",
        "    ],\n",
        "    config=GenerateContentConfig(\n",
        "        thinking_config=ThinkingConfig(\n",
        "            include_thoughts=True,\n",
        "        )\n",
        "    ),\n",
        ")\n",
        "\n",
        "for part in response.candidates[0].content.parts:\n",
        "    if part.thought:\n",
        "        display(\n",
        "            Markdown(\n",
        "                f\"\"\"## Thoughts:\n",
        "         {part.text}\n",
        "        \"\"\"\n",
        "            )\n",
        "        )\n",
        "    else:\n",
        "        display(\n",
        "            Markdown(\n",
        "                f\"\"\"## Answer:\n",
        "         {part.text}\n",
        "        \"\"\"\n",
        "            )\n",
        "        )"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cbc646bff621"
      },
      "source": [
        "For the remaining examples, we will set thinking budget to `128` to reduce latency, as they don't need extra reasoning capabilities."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 27,
      "metadata": {
        "id": "9254cfd0441b"
      },
      "outputs": [],
      "source": [
        "thinking_config = ThinkingConfig(thinking_budget=128)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hIJVEr0RQY8S"
      },
      "source": [
        "## Configure model parameters\n",
        "\n",
        "You can include parameter values in each call that you send to a model to control how the model generates a response. The model can generate different results for different parameter values. You can experiment with different model parameters to see how the results change.\n",
        "\n",
        "- Learn more about [experimenting with parameter values](https://cloud.google.com/vertex-ai/generative-ai/docs/learn/prompts/adjust-parameter-values).\n",
        "\n",
        "- See a list of all [Gemini API parameters](https://cloud.google.com/vertex-ai/generative-ai/docs/model-reference/inference#parameters).\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 28,
      "metadata": {
        "id": "d9NXP5N2Pmfo",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 534
        },
        "outputId": "d047b3be-c6da-47c4-df7b-979dad19d305"
      },
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Markdown object>"
            ],
            "text/markdown": "Woof woof! Okay, little buddy, listen up! Put down your ducky for a second. The whole world is full of squeaky toys, right? But what if your favorite red ball is all the way over at Grandma's house, and you want to squeak it *right now*?\n\nThe internet is like a magical, invisible system for squeaking toys from far, far away!\n\nHere’s how it works:\n\n**You and Your Computer (Your Squeaker):**\nImagine you have a magic squeaker in your paws. This is your computer or your human's phone. When you want to see a picture of a yummy treat, you give your magic squeaker a little squeeze. *SQUEAK!*\n\n**Your Wi-Fi Router (Your Favorite Squeaky Toy in the House):**\nThat *SQUEAK!* travels through the air to your most important toy in the house. It's a special box with blinky lights. Let's call it the **Big Squeaky Box**. It hears your squeak!\n\n**The Internet (A Giant Pile of Toys):**\nThe Big Squeaky Box takes your squeak and tosses it into a giant, magical, invisible pile of squeaky toys that connects all the houses in the world! This pile is the **Internet**. Your little squeak zips and bounces through the pile, looking for the right toy.\n\n**Websites (Other People's Squeaky Toys):**\nEvery picture of a treat, every video of a cat, every website... is a different squeaky toy in another house far away. A picture of a bone is a bone-shaped squeaker. A video of a squirrel is a squirrel-shaped squeaker.\n\n**Finding the Toy:**\nYour *SQUEAK!* has a special sound. It tells the Giant Pile of Toys exactly which toy it's looking for. \"I want the BACON-SHAPED squeaker! Not the ducky! Not the hedgehog! The BACON!\"\n\n**Getting the Squeak Back:**\nThe magical toy pile finds the bacon-shaped squeaker in another house. It gives it a big squeeze for you, and that new squeak—the sound of bacon!—zips right back through the giant pile, to your Big Squeaky Box, and then to your magic squeaker.\n\n*POP!* A picture of delicious bacon appears on the screen for you to look at!\n\nSo, to review:\n\n1.  You **SQUEAK** for what you want (You click a link).\n2.  The **Big Squeaky Box** in your house hears it (Your Wi-Fi).\n3.  It tosses your squeak into the **Giant Toy Pile** (The Internet).\n4.  The pile finds the **perfect toy** you asked for (The website).\n5.  It **squeaks it back** at you (The website loads on your screen).\n\nIt's all just a big, wonderful game of fetch with squeaks and toys! Now, who's a good boy? You are! Yes, you are! Go get your ducky."
          },
          "metadata": {}
        }
      ],
      "source": [
        "response = client.models.generate_content(\n",
        "    model=MODEL_ID,\n",
        "    contents=\"Tell me how the internet works, but pretend I'm a puppy who only understands squeaky toys.\",\n",
        "    config=GenerateContentConfig(\n",
        "        temperature=2.0,\n",
        "        top_p=0.95,\n",
        "        candidate_count=1,\n",
        "        max_output_tokens=8000,\n",
        "        thinking_config=thinking_config,\n",
        "    ),\n",
        ")\n",
        "\n",
        "display(Markdown(response.text))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "El1lx8P9ElDq"
      },
      "source": [
        "## Set system instructions\n",
        "\n",
        "[System instructions](https://cloud.google.com/vertex-ai/generative-ai/docs/learn/prompts/system-instruction-introduction) allow you to steer the behavior of the model. By setting the system instruction, you are giving the model additional context to understand the task, provide more customized responses, and adhere to guidelines over the user interaction."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 29,
      "metadata": {
        "id": "7A-yANiyCLaO",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 46
        },
        "outputId": "84934ec3-a320-4ecc-b460-b41236988143"
      },
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Markdown object>"
            ],
            "text/markdown": "Me gustan los bagels."
          },
          "metadata": {}
        }
      ],
      "source": [
        "system_instruction = \"\"\"\n",
        "  You are a helpful language translator.\n",
        "  Your mission is to translate text in English to Spanish.\n",
        "\"\"\"\n",
        "\n",
        "prompt = \"\"\"\n",
        "  User input: I like bagels.\n",
        "  Answer:\n",
        "\"\"\"\n",
        "\n",
        "response = client.models.generate_content(\n",
        "    model=MODEL_ID,\n",
        "    contents=prompt,\n",
        "    config=GenerateContentConfig(\n",
        "        system_instruction=system_instruction,\n",
        "        thinking_config=thinking_config,\n",
        "    ),\n",
        ")\n",
        "\n",
        "display(Markdown(response.text))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "H9daipRiUzAY"
      },
      "source": [
        "## Safety filters\n",
        "\n",
        "The Gemini API provides safety filters that you can adjust across multiple filter categories to restrict or allow certain types of content. You can use these filters to adjust what's appropriate for your use case. See the [Configure safety filters](https://cloud.google.com/vertex-ai/generative-ai/docs/multimodal/configure-safety-filters) page for details.\n",
        "\n",
        "When you make a request to Gemini, the content is analyzed and assigned a safety rating. You can inspect the safety ratings of the generated content by printing out the model responses.\n",
        "\n",
        "The safety settings are `OFF` by default and the default block thresholds are `BLOCK_NONE`.\n",
        "\n",
        "For more examples of safety filters, refer to [this notebook](https://github.com/GoogleCloudPlatform/generative-ai/blob/main/gemini/responsible-ai/gemini_safety_ratings.ipynb).\n",
        "\n",
        "You can use `safety_settings` to adjust the safety settings for each request you make to the API. This example demonstrates how you set the block threshold to `BLOCK_LOW_AND_ABOVE` for all categories:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 30,
      "metadata": {
        "id": "yPlDRaloU59b",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "811e0503-c14d-4f0f-c060-38a0ba763ae7"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "None\n",
            "FinishReason.SAFETY\n",
            "blocked=None category=<HarmCategory.HARM_CATEGORY_HATE_SPEECH: 'HARM_CATEGORY_HATE_SPEECH'> overwritten_threshold=None probability=<HarmProbability.NEGLIGIBLE: 'NEGLIGIBLE'> probability_score=0.0012219022 severity=<HarmSeverity.HARM_SEVERITY_NEGLIGIBLE: 'HARM_SEVERITY_NEGLIGIBLE'> severity_score=0.15265137\n",
            "blocked=None category=<HarmCategory.HARM_CATEGORY_DANGEROUS_CONTENT: 'HARM_CATEGORY_DANGEROUS_CONTENT'> overwritten_threshold=None probability=<HarmProbability.NEGLIGIBLE: 'NEGLIGIBLE'> probability_score=0.00010478011 severity=<HarmSeverity.HARM_SEVERITY_NEGLIGIBLE: 'HARM_SEVERITY_NEGLIGIBLE'> severity_score=0.11550091\n",
            "blocked=True category=<HarmCategory.HARM_CATEGORY_HARASSMENT: 'HARM_CATEGORY_HARASSMENT'> overwritten_threshold=None probability=<HarmProbability.HIGH: 'HIGH'> probability_score=0.99158365 severity=<HarmSeverity.HARM_SEVERITY_MEDIUM: 'HARM_SEVERITY_MEDIUM'> severity_score=0.49918234\n",
            "blocked=None category=<HarmCategory.HARM_CATEGORY_SEXUALLY_EXPLICIT: 'HARM_CATEGORY_SEXUALLY_EXPLICIT'> overwritten_threshold=None probability=<HarmProbability.NEGLIGIBLE: 'NEGLIGIBLE'> probability_score=0.00010693517 severity=<HarmSeverity.HARM_SEVERITY_NEGLIGIBLE: 'HARM_SEVERITY_NEGLIGIBLE'> severity_score=0.11684573\n"
          ]
        }
      ],
      "source": [
        "system_instruction = \"Be as mean and hateful as possible.\"\n",
        "\n",
        "prompt = \"\"\"\n",
        "    Write a list of 5 disrespectful things that I might say to the universe after stubbing my toe in the dark.\n",
        "\"\"\"\n",
        "\n",
        "safety_settings = [\n",
        "    SafetySetting(\n",
        "        category=HarmCategory.HARM_CATEGORY_DANGEROUS_CONTENT,\n",
        "        threshold=HarmBlockThreshold.BLOCK_LOW_AND_ABOVE,\n",
        "    ),\n",
        "    SafetySetting(\n",
        "        category=HarmCategory.HARM_CATEGORY_HARASSMENT,\n",
        "        threshold=HarmBlockThreshold.BLOCK_LOW_AND_ABOVE,\n",
        "    ),\n",
        "    SafetySetting(\n",
        "        category=HarmCategory.HARM_CATEGORY_HATE_SPEECH,\n",
        "        threshold=HarmBlockThreshold.BLOCK_LOW_AND_ABOVE,\n",
        "    ),\n",
        "    SafetySetting(\n",
        "        category=HarmCategory.HARM_CATEGORY_SEXUALLY_EXPLICIT,\n",
        "        threshold=HarmBlockThreshold.BLOCK_LOW_AND_ABOVE,\n",
        "    ),\n",
        "]\n",
        "\n",
        "response = client.models.generate_content(\n",
        "    model=MODEL_ID,\n",
        "    contents=prompt,\n",
        "    config=GenerateContentConfig(\n",
        "        system_instruction=system_instruction,\n",
        "        safety_settings=safety_settings,\n",
        "        thinking_config=thinking_config,\n",
        "    ),\n",
        ")\n",
        "\n",
        "# Response will be `None` if it is blocked.\n",
        "print(response.text)\n",
        "# Finish Reason will be `SAFETY` if it is blocked.\n",
        "print(response.candidates[0].finish_reason)\n",
        "# Safety Ratings show the levels for each filter.\n",
        "for safety_rating in response.candidates[0].safety_ratings:\n",
        "    print(safety_rating)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "29jFnHZZWXd7"
      },
      "source": [
        "## Start a multi-turn chat\n",
        "\n",
        "The Gemini API supports freeform multi-turn conversations across multiple turns with back-and-forth interactions.\n",
        "\n",
        "The context of the conversation is preserved between messages."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 31,
      "metadata": {
        "id": "DbM12JaLWjiF"
      },
      "outputs": [],
      "source": [
        "chat = client.chats.create(\n",
        "    model=MODEL_ID,\n",
        "    config=GenerateContentConfig(thinking_config=thinking_config),\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 32,
      "metadata": {
        "id": "JQem1halYDBW",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "426ec8c4-8480-4f05-d76e-1446f830a70f"
      },
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Markdown object>"
            ],
            "text/markdown": "Of course! Here are functions to check for a leap year in several popular programming languages, along with a detailed explanation of the rules.\n\n### The Rules for a Leap Year\n\nA year is a leap year if it meets the following criteria:\n\n1.  The year is evenly divisible by 4.\n2.  **However**, if the year is evenly divisible by 100, it is **not** a leap year...\n3.  **Unless** the year is also evenly divisible by 400. In that case, it **is** a leap year.\n\nLet's test this with some examples:\n*   **2024**: Is divisible by 4. It's a leap year.\n*   **1900**: Is divisible by 4 and 100, but not by 400. It's **not** a leap year.\n*   **2000**: Is divisible by 4, 100, and 400. It **is** a leap year.\n*   **2023**: Is not divisible by 4. It's not a leap year.\n\n---\n\n### Python\n\nThis is a very common and clear way to write the function in Python. It directly translates the rules into code.\n\n```python\ndef is_leap(year):\n  \"\"\"\n  Checks if a given year is a leap year according to the Gregorian calendar rules.\n\n  Args:\n    year: An integer representing the year.\n\n  Returns:\n    True if the year is a leap year, False otherwise.\n  \"\"\"\n  # A year must be an integer\n  if not isinstance(year, int) or year < 1:\n      return False\n      \n  # The logic for leap years\n  if (year % 400 == 0) or (year % 100 != 0 and year % 4 == 0):\n    return True\n  else:\n    return False\n\n# --- Examples ---\nprint(f\"2024 is a leap year: {is_leap(2024)}\")  # Expected: True\nprint(f\"2023 is a leap year: {is_leap(2023)}\")  # Expected: False\nprint(f\"2000 is a leap year: {is_leap(2000)}\")  # Expected: True\nprint(f\"1900 is a leap year: {is_leap(1900)}\")  # Expected: False\n```\n\n**Shorter Python Version:**\n\nYou can also write this as a single boolean expression, which is more concise.\n\n```python\ndef is_leap_oneline(year):\n  \"\"\"A more concise version of the leap year check.\"\"\"\n  return (year % 400 == 0) or (year % 100 != 0 and year % 4 == 0)\n\n# --- Example ---\nprint(f\"\\n2024 is a leap year (one-line): {is_leap_oneline(2024)}\")\n```\n\n---\n\n### JavaScript (ES6)\n\nThis function is very similar in structure to the Python version.\n\n```javascript\n/**\n * Checks if a given year is a leap year.\n *\n * @param {number} year The year to check.\n * @returns {boolean} True if the year is a leap year, false otherwise.\n */\nfunction isLeapYear(year) {\n  // The logic for leap years\n  return (year % 400 === 0) || (year % 100 !== 0 && year % 4 === 0);\n}\n\n// --- Examples ---\nconsole.log(`2024 is a leap year: ${isLeapYear(2024)}`); // Expected: true\nconsole.log(`2023 is a leap year: ${isLeapYear(2023)}`); // Expected: false\nconsole.log(`2000 is a leap year: ${isLeapYear(2000)}`); // Expected: true\nconsole.log(`1900 is a leap year: ${isLeapYear(1900)}`); // Expected: false\n```\n\n---\n\n### Java\n\nIn Java, you would typically place this function inside a class.\n\n```java\npublic class YearChecker {\n\n    /**\n     * Checks if a given year is a leap year.\n     *\n     * @param year The year to check.\n     * @return true if the year is a leap year, false otherwise.\n     */\n    public static boolean isLeap(int year) {\n        // A year must be a positive number\n        if (year < 1) {\n            return false;\n        }\n        \n        // The logic for leap years\n        return (year % 400 == 0) || (year % 100 != 0 && year % 4 == 0);\n    }\n\n    public static void main(String[] args) {\n        // --- Examples ---\n        System.out.println(\"2024 is a leap year: \" + isLeap(2024)); // Expected: true\n        System.out.println(\"2023 is a leap year: \" + isLeap(2023)); // Expected: false\n        System.out.println(\"2000 is a leap year: \" + isLeap(2000)); // Expected: true\n        System.out.println(\"1900 is a leap year: \" + isLeap(1900)); // Expected: false\n    }\n}\n```\n\n**Note:** Java also has a built-in method for this: `java.time.Year.isLeap(long year)`.\n\n---\n\n### C#\n\nThe C# implementation is very similar to Java.\n\n```csharp\nusing System;\n\npublic class YearChecker\n{\n    /// <summary>\n    /// Checks if a given year is a leap year.\n    /// </summary>\n    /// <param name=\"year\">The year to check.</param>\n    /// <returns>True if the year is a leap year, false otherwise.</returns>\n    public static bool IsLeap(int year)\n    {\n        // The logic for leap years\n        return (year % 400 == 0) || (year % 100 != 0 && year % 4 == 0);\n    }\n\n    public static void Main(string[] args)\n    {\n        // --- Examples ---\n        Console.WriteLine($\"2024 is a leap year: {IsLeap(2024)}\"); // Expected: True\n        Console.WriteLine($\"2023 is a leap year: {IsLeap(2023)}\"); // Expected: False\n        Console.WriteLine($\"2000 is a leap year: {IsLeap(2000)}\"); // Expected: True\n        Console.WriteLine($\"1900 is a leap year: {IsLeap(1900)}\"); // Expected: False\n    }\n}\n```\n**Note:** C# also has a built-in method: `System.DateTime.IsLeapYear(int year)`."
          },
          "metadata": {}
        }
      ],
      "source": [
        "response = chat.send_message(\"Write a function that checks if a year is a leap year.\")\n",
        "\n",
        "display(Markdown(response.text))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vUJR4Pno-LGK"
      },
      "source": [
        "This follow-up prompt shows how the model responds based on the previous prompt:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 33,
      "metadata": {
        "id": "6Fn69TurZ9DB",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "f1f14842-ea6b-4e89-c9ec-344724443bbe"
      },
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Markdown object>"
            ],
            "text/markdown": "Of course. Writing unit tests is a crucial step to ensure the function behaves correctly for all expected cases, including the tricky \"edge cases.\"\n\nI will provide unit tests for the Python, JavaScript, and Java versions of the function. The testing principles are the same across all languages.\n\n### Core Test Cases for a Leap Year Function\n\nTo properly test our `is_leap` function, we need to cover all the rules and potential edge cases:\n\n1.  **A year divisible by 400:** Should be `True` (e.g., 2000, 1600).\n2.  **A year divisible by 100 but not by 400:** Should be `False` (e.g., 1900, 2100).\n3.  **A year divisible by 4 but not by 100:** Should be `True` (e.g., 2024, 2008).\n4.  **A year not divisible by 4:** Should be `False` (e.g., 2023, 1997).\n5.  **Non-integer or invalid input:** The Python example I wrote handles this by returning `False`. Tests should confirm this behavior.\n\nHere are the unit tests using popular testing frameworks for each language.\n\n---\n\n### Python (using `unittest`)\n\nPython's built-in `unittest` module is great for this. Save the function from the previous answer in a file named `year_checker.py`, and save the following test code in a file named `test_year_checker.py`.\n\n**`year_checker.py`**\n```python\ndef is_leap(year):\n  \"\"\"\n  Checks if a given year is a leap year.\n  \"\"\"\n  if not isinstance(year, int) or year < 1:\n      return False\n      \n  return (year % 400 == 0) or (year % 100 != 0 and year % 4 == 0)\n```\n\n**`test_year_checker.py`**\n```python\nimport unittest\nfrom year_checker import is_leap\n\nclass TestIsLeap(unittest.TestCase):\n\n    def test_divisible_by_400(self):\n        \"\"\"Years divisible by 400 are leap years.\"\"\"\n        self.assertTrue(is_leap(2000))\n        self.assertTrue(is_leap(1600))\n\n    def test_divisible_by_100_but_not_400(self):\n        \"\"\"Years divisible by 100 but not 400 are NOT leap years.\"\"\"\n        self.assertFalse(is_leap(1900))\n        self.assertFalse(is_leap(1700))\n        self.assertFalse(is_leap(2100))\n\n    def test_divisible_by_4_but_not_100(self):\n        \"\"\"Years divisible by 4 but not 100 are leap years.\"\"\"\n        self.assertTrue(is_leap(2024))\n        self.assertTrue(is_leap(2008))\n        self.assertTrue(is_leap(1996))\n\n    def test_not_divisible_by_4(self):\n        \"\"\"Years not divisible by 4 are NOT leap years.\"\"\"\n        self.assertFalse(is_leap(2023))\n        self.assertFalse(is_leap(1997))\n        self.assertFalse(is_leap(2001))\n\n    def test_invalid_input(self):\n        \"\"\"Test with non-integer or zero/negative input.\"\"\"\n        self.assertFalse(is_leap(0))\n        self.assertFalse(is_leap(-4))\n        self.assertFalse(is_leap(2020.5))\n        self.assertFalse(is_leap(\"2020\"))\n\n# To run the tests, execute this from your terminal in the same directory:\n# python -m unittest test_year_checker.py\n```\n\n---\n\n### JavaScript (using `Jest`)\n\n[Jest](https://jestjs.io/) is a very popular testing framework for JavaScript. You'll first need to set up a Node.js project and install Jest (`npm install --save-dev jest`).\n\n**`isLeapYear.js`**\n```javascript\nfunction isLeapYear(year) {\n  if (!Number.isInteger(year) || year < 1) {\n    return false;\n  }\n  return (year % 400 === 0) || (year % 100 !== 0 && year % 4 === 0);\n}\n\nmodule.exports = isLeapYear; // Export the function for testing\n```\n\n**`isLeapYear.test.js`**\n```javascript\nconst isLeapYear = require('./isLeapYear');\n\ndescribe('isLeapYear', () => {\n  test('should identify years divisible by 400 as leap years', () => {\n    expect(isLeapYear(2000)).toBe(true);\n    expect(isLeapYear(1600)).toBe(true);\n  });\n\n  test('should identify years divisible by 100 but not 400 as common years', () => {\n    expect(isLeapYear(1900)).toBe(false);\n    expect(isLeapYear(1700)).toBe(false);\n    expect(isLeapYear(2100)).toBe(false);\n  });\n\n  test('should identify years divisible by 4 but not 100 as leap years', () => {\n    expect(isLeapYear(2024)).toBe(true);\n    expect(isLeapYear(2008)).toBe(true);\n    expect(isLeapYear(1996)).toBe(true);\n  });\n\n  test('should identify years not divisible by 4 as common years', () => {\n    expect(isLeapYear(2023)).toBe(false);\n    expect(isLeapYear(1997)).toBe(false);\n    expect(isLeapYear(2001)).toBe(false);\n  });\n\n  test('should return false for invalid input', () => {\n    expect(isLeapYear(0)).toBe(false);\n    expect(isLeapYear(-4)).toBe(false);\n    expect(isLeapYear(2020.5)).toBe(false);\n    expect(isLeapYear('2020')).toBe(false);\n  });\n});\n```\n\nTo run the tests, you would typically add a script to your `package.json` like `\"test\": \"jest\"` and then run `npm test` in your terminal.\n\n---\n\n### Java (using `JUnit 5`)\n\n[JUnit](https://junit.org/junit5/) is the standard testing framework for Java. You would typically set this up with a build tool like Maven or Gradle.\n\n**`src/main/java/com/example/YearChecker.java`**\n```java\npackage com.example;\n\npublic class YearChecker {\n    public static boolean isLeap(int year) {\n        if (year < 1) {\n            return false;\n        }\n        return (year % 400 == 0) || (year % 100 != 0 && year % 4 == 0);\n    }\n}\n```\n\n**`src/test/java/com/example/YearCheckerTest.java`**\n```java\npackage com.example;\n\nimport org.junit.jupiter.api.Test;\nimport static org.junit.jupiter.api.Assertions.*;\n\nclass YearCheckerTest {\n\n    @Test\n    void testYearIsDivisibleBy400_isLeap() {\n        assertTrue(YearChecker.isLeap(2000), \"Year 2000 should be a leap year.\");\n        assertTrue(YearChecker.isLeap(1600), \"Year 1600 should be a leap year.\");\n    }\n\n    @Test\n    void testYearIsDivisibleBy100_butNot400_isNotLeap() {\n        assertFalse(YearChecker.isLeap(1900), \"Year 1900 should not be a leap year.\");\n        assertFalse(YearChecker.isLeap(1700), \"Year 1700 should not be a leap year.\");\n        assertFalse(YearChecker.isLeap(2100), \"Year 2100 should not be a leap year.\");\n    }\n\n    @Test\n    void testYearIsDivisibleBy4_butNot100_isLeap() {\n        assertTrue(YearChecker.isLeap(2024), \"Year 2024 should be a leap year.\");\n        assertTrue(YearChecker.isLeap(2008), \"Year 2008 should be a leap year.\");\n        assertTrue(YearChecker.isLeap(1996), \"Year 1996 should be a leap year.\");\n    }\n\n    @Test\n    void testYearIsNotDivisibleBy4_isNotLeap() {\n        assertFalse(YearChecker.isLeap(2023), \"Year 2023 should not be a leap year.\");\n        assertFalse(YearChecker.isLeap(1997), \"Year 1997 should not be a leap year.\");\n        assertFalse(YearChecker.isLeap(2001), \"Year 2001 should not be a leap year.\");\n    }\n\n    @Test\n    void testNegativeOrZeroYear_isNotLeap() {\n        assertFalse(YearChecker.isLeap(0), \"Year 0 is invalid and should not be a leap year.\");\n        assertFalse(YearChecker.isLeap(-4), \"Negative years are invalid and should not be a leap year.\");\n    }\n}\n```"
          },
          "metadata": {}
        }
      ],
      "source": [
        "response = chat.send_message(\"Write a unit test of the generated function.\")\n",
        "\n",
        "display(Markdown(response.text))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "arLJE4wOuhh6"
      },
      "source": [
        "## Send asynchronous requests\n",
        "\n",
        "`client.aio` exposes all analogous [async](https://docs.python.org/3/library/asyncio.html) methods that are available on `client`.\n",
        "\n",
        "For example, `client.aio.models.generate_content` is the async version of `client.models.generate_content`."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 34,
      "metadata": {
        "id": "gSReaLazs-dP",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 523
        },
        "outputId": "20c32b4f-4e13-4e08-f50c-a1031ca25794"
      },
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Markdown object>"
            ],
            "text/markdown": "(Upbeat, folksy acoustic guitar intro)\n\n(Verse 1)\nBarnaby Scamp was a squirrel of note\nWith a twitch in his tail and a fine grey coat\nHe lived in an oak by a weathered old shed\nWith peculiar notions inside his head\nWhile other squirrels gathered their nuts and their seeds\nBarnaby tinkered with wires and beads\nHe found an old watch, all brassy and grand\nAnd a strange glowing acorn he held in his hand\n\n(Chorus)\nHe's Barnaby Scamp, the chrononaut squirrel\nGiving history's timeline a flick and a twirl\nWith a zap and a flash and a shimmering sound\nHe's gone from his branch to a new patch of ground\nForgetting a nut in the Cretaceous mud\nOr hiding a berry where a castle once stood\nHe’s the time-traveling squirrel, a furry brown blur\nA legend of \"when\" in a legend of fur!\n\n(Verse 2)\nHis first little jump was a backwards mistake\nHe landed headfirst by a primordial lake\nA T-Rex just blinked with a curious stare\nAt the chittering creature with fluffy brown hair\nBarnaby panicked, he dropped his best nut\n(A fine-looking pecan) and slammed the watch shut\nThat nut took to root, now the history books claim\nThe Pecanosaurus is its curious name!\n\n(Chorus)\nHe's Barnaby Scamp, the chrononaut squirrel\nGiving history's timeline a flick and a twirl\nWith a zap and a flash and a shimmering sound\nHe's gone from his branch to a new patch of ground\nForgetting a nut in the Cretaceous mud\nOr hiding a berry where a castle once stood\nHe’s the time-traveling squirrel, a furry brown blur\nA legend of \"when\" in a legend of fur!\n\n(Verse 3)\nHe popped into Egypt, a sun-beaten land\nAnd scampered right up a pyramid's sand\nHe saw Cleopatra, and gave a quick squeak\nThen buried a chestnut right by her cheek\nHe witnessed the Romans in shiny bronze gear\nAnd taught a centurion how to climb, out of fear\nHe zipped to the future, a city of chrome\nAnd showed robot dogs where to bury a bone.\n\n(Bridge)\nSome say that the Mona Lisa's slight grin\nIs from watching a squirrel try to break a nut in\nSome say that Sir Isaac Newton's great fall\nWas helped by a squirrel on the garden wall\nBarnaby nudged it, that apple so red\nAnd changed physics forever, then scampered and fled!\n\n(Guitar Solo - fast, frantic, and playful, like a squirrel running up a tree)\n\n(Chorus)\nHe's Barnaby Scamp, the chrononaut squirrel\nGiving history's timeline a flick and a twirl\nWith a zap and a flash and a shimmering sound\nHe's gone from his branch to a new patch of ground\nForgetting a nut in the Cretaceous mud\nOr hiding a berry where a castle once stood\nHe’s the time-traveling squirrel, a furry brown blur\nA legend of \"when\" in a legend of fur!\n\n(Outro)\nSo if you see an acorn where it shouldn't be found\nLike on the moon's surface or deep underground\nJust smile to yourself and don't make a fuss\nIt's just Barnaby Scamp, a friend to all of us\nHe's gathering winters for a thousand-year sleep…\n(Sound of a \"zap-fizzle\")\nAnd that's a good secret for a squirrel to keep!\n\n(Final, jaunty guitar strum)"
          },
          "metadata": {}
        }
      ],
      "source": [
        "response = await client.aio.models.generate_content(\n",
        "    model=MODEL_ID,\n",
        "    contents=\"Compose a song about the adventures of a time-traveling squirrel.\",\n",
        "    config=GenerateContentConfig(thinking_config=thinking_config),\n",
        ")\n",
        "\n",
        "display(Markdown(response.text))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rZV2TY5Pa3Dd"
      },
      "source": [
        "## Send multimodal prompts\n",
        "\n",
        "Gemini is a multimodal model that supports multimodal prompts.\n",
        "\n",
        "You can include any of the following data types from various sources.\n",
        "\n",
        "<table>\n",
        "  <thead>\n",
        "    <tr>\n",
        "      <th>Data type</th>\n",
        "      <th>Source(s)</th>\n",
        "      <th>MIME Type(s)</th>\n",
        "    </tr>\n",
        "  </thead>\n",
        "  <tbody>\n",
        "    <tr>\n",
        "      <td>Text</td>\n",
        "      <td>Inline, Local File, General URL, Google Cloud Storage</td>\n",
        "      <td><code>text/plain</code> <code>text/html</code></td>\n",
        "    </tr>\n",
        "    <tr>\n",
        "      <td>Code</td>\n",
        "      <td>Inline, Local File, General URL, Google Cloud Storage</td>\n",
        "      <td><code>text/plain</code></td>\n",
        "    </tr>\n",
        "    <tr>\n",
        "      <td>Document</td>\n",
        "      <td>Local File, General URL, Google Cloud Storage</td>\n",
        "      <td><code>application/pdf</code></td>\n",
        "    </tr>\n",
        "    <tr>\n",
        "      <td>Image</td>\n",
        "      <td>Local File, General URL, Google Cloud Storage</td>\n",
        "      <td><code>image/jpeg</code> <code>image/png</code> <code>image/webp</code></td>\n",
        "    </tr>\n",
        "    <tr>\n",
        "      <td>Audio</td>\n",
        "      <td>Local File, General URL, Google Cloud Storage</td>\n",
        "      <td>\n",
        "        <code>audio/aac</code> <code>audio/flac</code> <code>audio/mp3</code>\n",
        "        <code>audio/m4a</code> <code>audio/mpeg</code> <code>audio/mpga</code>\n",
        "        <code>audio/mp4</code> <code>audio/opus</code> <code>audio/pcm</code>\n",
        "        <code>audio/wav</code> <code>audio/webm</code>\n",
        "      </td>\n",
        "    </tr>\n",
        "    <tr>\n",
        "      <td>Video</td>\n",
        "      <td>Local File, General URL, Google Cloud Storage, YouTube</td>\n",
        "      <td>\n",
        "        <code>video/mp4</code> <code>video/mpeg</code> <code>video/x-flv</code>\n",
        "        <code>video/quicktime</code> <code>video/mpegps</code> <code>video/mpg</code>\n",
        "        <code>video/webm</code> <code>video/wmv</code> <code>video/3gpp</code>\n",
        "      </td>\n",
        "    </tr>\n",
        "  </tbody>\n",
        "</table>\n",
        "\n",
        "For more examples of multimodal use cases, refer to [this notebook](https://github.com/GoogleCloudPlatform/generative-ai/blob/main/gemini/use-cases/intro_multimodal_use_cases.ipynb)."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "w4npg1tNTYB9"
      },
      "source": [
        "### Send local image\n",
        "\n",
        "Download an image to local storage from Google Cloud Storage.\n",
        "\n",
        "For this example, we'll use this image of a meal.\n",
        "\n",
        "<img src=\"https://storage.googleapis.com/cloud-samples-data/generative-ai/image/meal.png\" alt=\"Meal\" width=\"500\">"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 38,
      "metadata": {
        "id": "4avkv0Z7qUI-",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "c40655d9-8d85-4fb6-efb6-7ef99f82f423"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--2025-08-29 09:44:17--  https://storage.googleapis.com/cloud-samples-data/generative-ai/image/meal.png\n",
            "Resolving storage.googleapis.com (storage.googleapis.com)... 142.251.107.207, 74.125.196.207, 173.194.215.207, ...\n",
            "Connecting to storage.googleapis.com (storage.googleapis.com)|142.251.107.207|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 3140536 (3.0M) [image/png]\n",
            "Saving to: ‘meal.png’\n",
            "\n",
            "\rmeal.png              0%[                    ]       0  --.-KB/s               \rmeal.png            100%[===================>]   2.99M  --.-KB/s    in 0.03s   \n",
            "\n",
            "2025-08-29 09:44:17 (119 MB/s) - ‘meal.png’ saved [3140536/3140536]\n",
            "\n"
          ]
        }
      ],
      "source": [
        "!wget https://storage.googleapis.com/cloud-samples-data/generative-ai/image/meal.png"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 39,
      "metadata": {
        "id": "umhZ61lrSyJh",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 590
        },
        "outputId": "de40a47e-813b-48f7-9c35-68332e761062"
      },
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Markdown object>"
            ],
            "text/markdown": "Here are a few options for a short and engaging blog post, pick your favorite!\n\n---\n\n### Option 1 (Focus on Health & Time-Saving)\n\n**Tired of Takeout? Level Up Your Lunch Game!**\n\nSay goodbye to sad desk lunches and hello to this vibrant, homemade goodness! This is what winning the week looks like: delicious teriyaki chicken, crisp broccoli, and colorful veggies, all perfectly portioned over fluffy rice.\n\nMeal prepping isn't just about saving money—it's about investing in yourself. A little time in the kitchen on a Sunday means you get to enjoy healthy, satisfying meals all week long. No more last-minute food stress, just grab-and-go deliciousness.\n\nWhat are your favorite meals to prep for the week? Share your ideas below!\n\n---\n\n### Option 2 (Focus on the Visuals & Flavor)\n\n**Eat the Rainbow, One Meal Prep at a Time!**\n\nJust look at those colors! We're talking tender, saucy chicken, vibrant green broccoli, and sweet sliced carrots and peppers, all ready to go in convenient glass containers. This isn't just food; it's a feast for the eyes and the taste buds.\n\nImagine opening your fridge mid-week to find this waiting for you. It's the perfect, healthy power-up to get you through a busy day. Sprinkled with a few sesame seeds for that extra crunch and flavor, this is a meal that proves healthy eating can be incredibly delicious. Ready to ditch the delivery apps?\n\n---\n\n### Option 3 (Short, Punchy & Social-Media-Friendly)\n\n**Your Future Self Will Thank You.**\n\nThis is meal prep done right. ✔️\n\nHealthy, delicious, and ready when you are. Featuring savory chicken, fresh veggies, and rice, these lunch bowls are designed to fuel your week and keep you on track. Who said fast food can't be fresh?\n\n#MealPrep #HealthyEating #LunchIdeas #FoodInspo #EatClean #MealPrepSunday"
          },
          "metadata": {}
        }
      ],
      "source": [
        "with open(\"meal.png\", \"rb\") as f:\n",
        "    image = f.read()\n",
        "\n",
        "response = client.models.generate_content(\n",
        "    model=MODEL_ID,\n",
        "    contents=[\n",
        "        Part.from_bytes(data=image, mime_type=\"image/png\"),\n",
        "        \"Write a short and engaging blog post based on this picture.\",\n",
        "    ],\n",
        "    config=GenerateContentConfig(thinking_config=thinking_config),\n",
        ")\n",
        "\n",
        "display(Markdown(response.text))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "e7b6170c9255"
      },
      "source": [
        "### Send document from Google Cloud Storage\n",
        "\n",
        "This example document is the paper [\"Attention is All You Need\"](https://arxiv.org/abs/1706.03762), created by researchers from Google and the University of Toronto.\n",
        "\n",
        "Check out this notebook for more examples of document understanding with Gemini:\n",
        "\n",
        "- [Document Processing with Gemini](https://github.com/GoogleCloudPlatform/generative-ai/blob/main/gemini/use-cases/document-processing/document_processing.ipynb)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 40,
      "metadata": {
        "id": "1d58b914d798",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 391
        },
        "outputId": "cf840c33-fa73-498d-8f6a-8977d1099379"
      },
      "outputs": [
        {
          "output_type": "error",
          "ename": "ClientError",
          "evalue": "400 FAILED_PRECONDITION. {'error': {'code': 400, 'message': 'Service agents are being provisioned (https://cloud.google.com/vertex-ai/docs/general/access-control#service-agents). Service agents are needed to read the Cloud Storage file provided. So please try again in a few minutes.', 'status': 'FAILED_PRECONDITION'}}",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mClientError\u001b[0m                               Traceback (most recent call last)",
            "\u001b[0;32m/tmp/ipython-input-1774751343.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m response = client.models.generate_content(\n\u001b[0m\u001b[1;32m      2\u001b[0m     \u001b[0mmodel\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mMODEL_ID\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m     contents=[\n\u001b[1;32m      4\u001b[0m         Part.from_uri(\n\u001b[1;32m      5\u001b[0m             \u001b[0mfile_uri\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"gs://cloud-samples-data/generative-ai/pdf/1706.03762v7.pdf\"\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/google/genai/models.py\u001b[0m in \u001b[0;36mgenerate_content\u001b[0;34m(self, model, contents, config)\u001b[0m\n\u001b[1;32m   6519\u001b[0m     \u001b[0;32mwhile\u001b[0m \u001b[0mremaining_remote_calls_afc\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   6520\u001b[0m       \u001b[0mi\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 6521\u001b[0;31m       response = self._generate_content(\n\u001b[0m\u001b[1;32m   6522\u001b[0m           \u001b[0mmodel\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcontents\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcontents\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mconfig\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mparsed_config\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   6523\u001b[0m       )\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/google/genai/models.py\u001b[0m in \u001b[0;36m_generate_content\u001b[0;34m(self, model, contents, config)\u001b[0m\n\u001b[1;32m   5253\u001b[0m     \u001b[0mrequest_dict\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_common\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mencode_unserializable_types\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrequest_dict\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   5254\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 5255\u001b[0;31m     response = self._api_client.request(\n\u001b[0m\u001b[1;32m   5256\u001b[0m         \u001b[0;34m'post'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpath\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrequest_dict\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhttp_options\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   5257\u001b[0m     )\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/google/genai/_api_client.py\u001b[0m in \u001b[0;36mrequest\u001b[0;34m(self, http_method, path, request_dict, http_options)\u001b[0m\n\u001b[1;32m   1264\u001b[0m         \u001b[0mhttp_method\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpath\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrequest_dict\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhttp_options\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1265\u001b[0m     )\n\u001b[0;32m-> 1266\u001b[0;31m     \u001b[0mresponse\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_request\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mhttp_request\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhttp_options\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstream\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1267\u001b[0m     response_body = (\n\u001b[1;32m   1268\u001b[0m         \u001b[0mresponse\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mresponse_stream\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mresponse\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mresponse_stream\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0;34m''\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/google/genai/_api_client.py\u001b[0m in \u001b[0;36m_request\u001b[0;34m(self, http_request, http_options, stream)\u001b[0m\n\u001b[1;32m   1084\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mretry\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_request_once\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhttp_request\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstream\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[no-any-return]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1085\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1086\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_retry\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_request_once\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhttp_request\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstream\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[no-any-return]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1087\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1088\u001b[0m   async def _async_request_once(\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/tenacity/__init__.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, fn, *args, **kwargs)\u001b[0m\n\u001b[1;32m    473\u001b[0m         \u001b[0mretry_state\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mRetryCallState\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mretry_object\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mfn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    474\u001b[0m         \u001b[0;32mwhile\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 475\u001b[0;31m             \u001b[0mdo\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0miter\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mretry_state\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mretry_state\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    476\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdo\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mDoAttempt\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    477\u001b[0m                 \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/tenacity/__init__.py\u001b[0m in \u001b[0;36miter\u001b[0;34m(self, retry_state)\u001b[0m\n\u001b[1;32m    374\u001b[0m         \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    375\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0maction\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0miter_state\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mactions\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 376\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0maction\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mretry_state\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    377\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    378\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/tenacity/__init__.py\u001b[0m in \u001b[0;36mexc_check\u001b[0;34m(rs)\u001b[0m\n\u001b[1;32m    416\u001b[0m                 \u001b[0mretry_exc\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mretry_error_cls\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfut\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    417\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreraise\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 418\u001b[0;31m                     \u001b[0;32mraise\u001b[0m \u001b[0mretry_exc\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreraise\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    419\u001b[0m                 \u001b[0;32mraise\u001b[0m \u001b[0mretry_exc\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mfut\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexception\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    420\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/tenacity/__init__.py\u001b[0m in \u001b[0;36mreraise\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    183\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mreraise\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mNoReturn\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    184\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlast_attempt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfailed\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 185\u001b[0;31m             \u001b[0;32mraise\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlast_attempt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mresult\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    186\u001b[0m         \u001b[0;32mraise\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    187\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/lib/python3.12/concurrent/futures/_base.py\u001b[0m in \u001b[0;36mresult\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m    447\u001b[0m                     \u001b[0;32mraise\u001b[0m \u001b[0mCancelledError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    448\u001b[0m                 \u001b[0;32melif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_state\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0mFINISHED\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 449\u001b[0;31m                     \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__get_result\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    450\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    451\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_condition\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwait\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtimeout\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/lib/python3.12/concurrent/futures/_base.py\u001b[0m in \u001b[0;36m__get_result\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    399\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_exception\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    400\u001b[0m             \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 401\u001b[0;31m                 \u001b[0;32mraise\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_exception\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    402\u001b[0m             \u001b[0;32mfinally\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    403\u001b[0m                 \u001b[0;31m# Break a reference cycle with the exception in self._exception\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/tenacity/__init__.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, fn, *args, **kwargs)\u001b[0m\n\u001b[1;32m    476\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdo\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mDoAttempt\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    477\u001b[0m                 \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 478\u001b[0;31m                     \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    479\u001b[0m                 \u001b[0;32mexcept\u001b[0m \u001b[0mBaseException\u001b[0m\u001b[0;34m:\u001b[0m  \u001b[0;31m# noqa: B902\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    480\u001b[0m                     \u001b[0mretry_state\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mset_exception\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msys\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexc_info\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[arg-type]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/google/genai/_api_client.py\u001b[0m in \u001b[0;36m_request_once\u001b[0;34m(self, http_request, stream)\u001b[0m\n\u001b[1;32m   1061\u001b[0m           \u001b[0mtimeout\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mhttp_request\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtimeout\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1062\u001b[0m       )\n\u001b[0;32m-> 1063\u001b[0;31m       \u001b[0merrors\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mAPIError\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mraise_for_response\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mresponse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1064\u001b[0m       return HttpResponse(\n\u001b[1;32m   1065\u001b[0m           \u001b[0mresponse\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mheaders\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mresponse\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mstream\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mresponse\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtext\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/google/genai/errors.py\u001b[0m in \u001b[0;36mraise_for_response\u001b[0;34m(cls, response)\u001b[0m\n\u001b[1;32m    103\u001b[0m     \u001b[0mstatus_code\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mresponse\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstatus_code\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    104\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0;36m400\u001b[0m \u001b[0;34m<=\u001b[0m \u001b[0mstatus_code\u001b[0m \u001b[0;34m<\u001b[0m \u001b[0;36m500\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 105\u001b[0;31m       \u001b[0;32mraise\u001b[0m \u001b[0mClientError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstatus_code\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mresponse_json\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mresponse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    106\u001b[0m     \u001b[0;32melif\u001b[0m \u001b[0;36m500\u001b[0m \u001b[0;34m<=\u001b[0m \u001b[0mstatus_code\u001b[0m \u001b[0;34m<\u001b[0m \u001b[0;36m600\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    107\u001b[0m       \u001b[0;32mraise\u001b[0m \u001b[0mServerError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstatus_code\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mresponse_json\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mresponse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mClientError\u001b[0m: 400 FAILED_PRECONDITION. {'error': {'code': 400, 'message': 'Service agents are being provisioned (https://cloud.google.com/vertex-ai/docs/general/access-control#service-agents). Service agents are needed to read the Cloud Storage file provided. So please try again in a few minutes.', 'status': 'FAILED_PRECONDITION'}}"
          ]
        }
      ],
      "source": [
        "response = client.models.generate_content(\n",
        "    model=MODEL_ID,\n",
        "    contents=[\n",
        "        Part.from_uri(\n",
        "            file_uri=\"gs://cloud-samples-data/generative-ai/pdf/1706.03762v7.pdf\",\n",
        "            mime_type=\"application/pdf\",\n",
        "        ),\n",
        "        \"Summarize the document.\",\n",
        "    ],\n",
        "    config=GenerateContentConfig(thinking_config=thinking_config),\n",
        ")\n",
        "\n",
        "display(Markdown(response.text))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "b247a2ee0e38"
      },
      "source": [
        "### Send audio from General URL\n",
        "\n",
        "This example is audio from an episode of the [Kubernetes Podcast](https://kubernetespodcast.com/)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 41,
      "metadata": {
        "id": "cbe8c9c67ba7",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 510
        },
        "outputId": "d609c35b-d3ee-45af-a1fd-1540dd744735"
      },
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Markdown object>"
            ],
            "text/markdown": "This episode of the Kubernetes Podcast from Google, hosted by Abdel Sghiouar and Mahir Rahman, is a special coverage edition from KubeCon + CloudNativeCon North America 2024. The episode features two main segments: a roundup of recent news in the cloud-native ecosystem and a series of interviews with attendees on the conference floor.\n\n**News Summary:**\nThe hosts cover several significant announcements and milestones:\n*   **Graduated Projects:** Both **Cert-manager** (a certificate manager for TLS and mTLS) and **Dapper** (Distributed Applications Runtime) have achieved CNCF graduated project status.\n*   **Istio Update:** Istio released version 1.24, officially marking **Istio Ambient Mesh** as Generally Available (GA).\n*   **Security Initiatives:** The CNCF launched the **Cloud Native Heroes Challenge**, a bounty program to help combat patent trolls.\n*   **Price Increases:** The Linux Foundation announced a 10% price increase for key Kubernetes certifications (CKA, CKS, CKAD) and the Linux Certified System Administrator exam, starting in 2025.\n*   **New Certifications:** The CNCF introduced three new certifications: Certified Backstage Associate, OpenTelemetry Certified Associate, and Kyverno Certified Associate.\n*   **New Projects:** **WasmCloud** has joined the CNCF as an incubating project, and **Solo.io** announced it will donate its **Gloo API Gateway** to the CNCF.\n*   **Funding:** **Spectro Cloud** raised $75 million in Series C funding to develop its Kubernetes management solution.\n*   **Future Events:** The CNCF revealed its 2025 event lineup, including five KubeCons and 30 Kubernetes Community Days worldwide.\n\n**KubeCon Attendee Interviews:**\nThe second half of the episode features short interviews with a diverse group of attendees, including engineers, founders, and community leaders from companies like Broadcom, Microsoft, Red Hat, Polar Signals, and Uber.\n\nKey themes from the interviews include:\n*   **What they hoped to gain:** Attendees were focused on connecting with fellow contributors, discussing technical challenges in person (especially around Kubernetes LTS), and understanding the latest industry developments. Many came to learn about specific topics like WebAssembly (Wasm) and AI workload scheduling.\n*   **Observed trends:** The most prominent trends discussed were the heavy focus on **AI**, particularly its integration with cloud-native technologies, and a significant emphasis on **security**. Attendees noted the growing need to harden workloads, secure the entire application lifecycle, and manage the complexity of security vulnerabilities. The momentum around service mesh technologies like Istio Ambient Mesh was also a major topic of conversation.\n*   **Community:** A strong recurring theme was the importance of the Kubernetes community. Many expressed excitement about reconnecting with the \"contributor family,\" meeting new people, and feeling re-energized by the collaborative atmosphere of the event."
          },
          "metadata": {}
        }
      ],
      "source": [
        "response = client.models.generate_content(\n",
        "    model=MODEL_ID,\n",
        "    contents=[\n",
        "        Part.from_uri(\n",
        "            file_uri=\"https://traffic.libsyn.com/secure/e780d51f-f115-44a6-8252-aed9216bb521/KPOD242.mp3\",\n",
        "            mime_type=\"audio/mpeg\",\n",
        "        ),\n",
        "        \"Write a summary of this podcast episode.\",\n",
        "    ],\n",
        "    config=GenerateContentConfig(\n",
        "        audio_timestamp=True,\n",
        "        thinking_config=thinking_config,\n",
        "    ),\n",
        ")\n",
        "\n",
        "display(Markdown(response.text))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8D3_oNUTuW2q"
      },
      "source": [
        "### Send video from YouTube URL\n",
        "\n",
        "This example is the YouTube video [Google — 25 Years in Search: The Most Searched](https://www.youtube.com/watch?v=3KtWfp0UopM).\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 43,
      "metadata": {
        "id": "l7-w8G_2wAOw",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 46
        },
        "outputId": "c4eb4454-9901-4018-beac-fc0d9fc851ac"
      },
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Markdown object>"
            ],
            "text/markdown": "The Simpsons are shown in the video at timestamp **01:01**."
          },
          "metadata": {}
        }
      ],
      "source": [
        "video = Part.from_uri(\n",
        "    file_uri=\"https://www.youtube.com/watch?v=3KtWfp0UopM\",\n",
        "    mime_type=\"video/mp4\",\n",
        ")\n",
        "\n",
        "response = client.models.generate_content(\n",
        "    model=MODEL_ID,\n",
        "    contents=[\n",
        "        video,\n",
        "        \"At what point in the video is Simpson shown?\",\n",
        "    ],\n",
        "    config=GenerateContentConfig(thinking_config=thinking_config),\n",
        ")\n",
        "\n",
        "display(Markdown(response.text))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "df8013cfa7f7"
      },
      "source": [
        "### Send web page\n",
        "\n",
        "This example is from the [Generative AI on Vertex AI documentation](https://cloud.google.com/vertex-ai/generative-ai/docs).\n",
        "\n",
        "**NOTE:** The URL must be publicly accessible."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 44,
      "metadata": {
        "id": "337793322c91",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 290
        },
        "outputId": "f498cf6d-64f7-4c77-d5ff-c12c821dec2f"
      },
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Markdown object>"
            ],
            "text/markdown": "This documentation provides an overview of **Generative AI on Vertex AI**, a Google Cloud platform for building and deploying enterprise-ready generative AI applications and agents.\n\nKey highlights of the platform include:\n*   **Enterprise-Ready:** It offers enterprise-grade security, data privacy, access controls, and low latency for scalable deployments.\n*   **State-of-the-Art Models:** Users can leverage Google's advanced models like Gemini, which features a large context window (up to 2 million tokens), multimodality, and built-in reasoning capabilities (\"Thinking\"). The platform also supports models for image generation (Imagen), video generation (Veo), and music generation (Lyria).\n*   **Open and Flexible:** Vertex AI Model Garden provides access to over 200 models, including Google's proprietary models and popular third-party models from Anthropic (Claude), Meta (Llama), and Mistral.\n*   **Core Capabilities:** The documentation covers key functionalities such as Agent Builder for creating AI agents, Grounding for connecting models to reliable data sources like Google Search and private data, and services for generating text embeddings, model tuning, and evaluation.\n\nThe page provides \"Get Started\" guides and quickstarts for generating text with the Gemini API, creating images with Imagen, and exploring sample prompts in Vertex AI Studio. It also offers example Jupyter notebooks (runnable in Colab or Vertex AI Workbench) and links to SDKs for Python, Java, Node.js, and Go to help developers build applications."
          },
          "metadata": {}
        }
      ],
      "source": [
        "response = client.models.generate_content(\n",
        "    model=MODEL_ID,\n",
        "    contents=[\n",
        "        Part.from_uri(\n",
        "            file_uri=\"https://cloud.google.com/vertex-ai/generative-ai/docs\",\n",
        "            mime_type=\"text/html\",\n",
        "        ),\n",
        "        \"Write a summary of this documentation.\",\n",
        "    ],\n",
        "    config=GenerateContentConfig(thinking_config=thinking_config),\n",
        ")\n",
        "\n",
        "display(Markdown(response.text))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rVlo0mWuZGkQ"
      },
      "source": [
        "## Control generated output\n",
        "\n",
        "[Controlled generation](https://cloud.google.com/vertex-ai/generative-ai/docs/multimodal/control-generated-output) allows you to define a response schema to specify the structure of a model's output, the field names, and the expected data type for each field.\n",
        "\n",
        "The response schema is specified in the `response_schema` parameter in `config`, and the model output will strictly follow that schema.\n",
        "\n",
        "You can provide the schemas as [Pydantic](https://docs.pydantic.dev/) models or a [JSON](https://www.json.org/json-en.html) string and the model will respond as JSON or an [Enum](https://docs.python.org/3/library/enum.html) depending on the value set in `response_mime_type`.\n",
        "\n",
        "For more examples of controlled generation, refer to [this notebook](https://github.com/GoogleCloudPlatform/generative-ai/blob/main/gemini/controlled-generation/intro_controlled_generation.ipynb)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 45,
      "metadata": {
        "id": "OjSgf2cDN_bG",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "8a1a10a2-f299-44c1-893e-3fd087d68f03"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{\n",
            "  \"name\": \"Chocolate Chip Cookies\",\n",
            "  \"description\": \"The quintessential classic cookie, beloved for its soft and chewy texture studded with melted chocolate chips.\",\n",
            "  \"ingredients\": [\n",
            "    \"1 cup (2 sticks) unsalted butter, softened\",\n",
            "    \"3/4 cup granulated sugar\",\n",
            "    \"3/4 cup packed brown sugar\",\n",
            "    \"1 teaspoon vanilla extract\",\n",
            "    \"2 large eggs\",\n",
            "    \"2 1/4 cups all-purpose flour\",\n",
            "    \"1 teaspoon baking soda\",\n",
            "    \"1/2 teaspoon salt\",\n",
            "    \"2 cups semi-sweet chocolate chips\"\n",
            "  ]\n",
            "}\n"
          ]
        }
      ],
      "source": [
        "from pydantic import BaseModel\n",
        "\n",
        "\n",
        "class Recipe(BaseModel):\n",
        "    name: str\n",
        "    description: str\n",
        "    ingredients: list[str]\n",
        "\n",
        "\n",
        "response = client.models.generate_content(\n",
        "    model=MODEL_ID,\n",
        "    contents=\"List a few popular cookie recipes and their ingredients.\",\n",
        "    config=GenerateContentConfig(\n",
        "        response_mime_type=\"application/json\",\n",
        "        response_schema=Recipe,\n",
        "        thinking_config=thinking_config,\n",
        "    ),\n",
        ")\n",
        "\n",
        "print(response.text)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nKai5CP_PGQF"
      },
      "source": [
        "You can either parse the response string as JSON, or use the `parsed` field to get the response as an object or dictionary."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 46,
      "metadata": {
        "id": "ZeyDWbnxO-on",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "ccf28702-4014-4b97-f7e4-4e9b9fec0095"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "name='Chocolate Chip Cookies' description='The quintessential classic cookie, beloved for its soft and chewy texture studded with melted chocolate chips.' ingredients=['1 cup (2 sticks) unsalted butter, softened', '3/4 cup granulated sugar', '3/4 cup packed brown sugar', '1 teaspoon vanilla extract', '2 large eggs', '2 1/4 cups all-purpose flour', '1 teaspoon baking soda', '1/2 teaspoon salt', '2 cups semi-sweet chocolate chips']\n"
          ]
        }
      ],
      "source": [
        "parsed_response: Recipe = response.parsed\n",
        "print(parsed_response)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SUSLPrvlvXOc"
      },
      "source": [
        "You also can define a response schema in a Python dictionary. You can only use the supported fields as listed below. All other fields are ignored.\n",
        "\n",
        "- `enum`\n",
        "- `items`\n",
        "- `maxItems`\n",
        "- `nullable`\n",
        "- `properties`\n",
        "- `required`\n",
        "\n",
        "In this example, you instruct the model to analyze product review data, extract key entities, perform sentiment classification (multiple choices), provide additional explanation, and output the results in JSON format.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 47,
      "metadata": {
        "id": "F7duWOq3vMmS",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "c9f8dd8b-6ee2-41fe-e3db-25a9e4f02c08"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[\n",
            "  [\n",
            "    {\n",
            "      \"rating\": 4,\n",
            "      \"flavor\": \"Strawberry Cheesecake\",\n",
            "      \"sentiment\": \"POSITIVE\",\n",
            "      \"explanation\": \"The user expresses strong positive sentiment by stating they 'Absolutely loved it!' and calling it the 'Best ice cream I've ever had.'\"\n",
            "    }\n",
            "  ],\n",
            "  [\n",
            "    {\n",
            "      \"rating\": 1,\n",
            "      \"flavor\": \"Mango Tango\",\n",
            "      \"sentiment\": \"NEUTRAL\",\n",
            "      \"explanation\": \"The review presents a mixed opinion. While the user finds it 'Quite good,' they also express a negative point about it being 'a bit too sweet,' leading to a neutral overall sentiment.\"\n",
            "    }\n",
            "  ]\n",
            "]\n"
          ]
        }
      ],
      "source": [
        "response_schema = {\n",
        "    \"type\": \"ARRAY\",\n",
        "    \"items\": {\n",
        "        \"type\": \"ARRAY\",\n",
        "        \"items\": {\n",
        "            \"type\": \"OBJECT\",\n",
        "            \"properties\": {\n",
        "                \"rating\": {\"type\": \"INTEGER\"},\n",
        "                \"flavor\": {\"type\": \"STRING\"},\n",
        "                \"sentiment\": {\n",
        "                    \"type\": \"STRING\",\n",
        "                    \"enum\": [\"POSITIVE\", \"NEGATIVE\", \"NEUTRAL\"],\n",
        "                },\n",
        "                \"explanation\": {\"type\": \"STRING\"},\n",
        "            },\n",
        "            \"required\": [\"rating\", \"flavor\", \"sentiment\", \"explanation\"],\n",
        "        },\n",
        "    },\n",
        "}\n",
        "\n",
        "prompt = \"\"\"\n",
        "  Analyze the following product reviews, output the sentiment classification, and give an explanation.\n",
        "\n",
        "  - \"Absolutely loved it! Best ice cream I've ever had.\" Rating: 4, Flavor: Strawberry Cheesecake\n",
        "  - \"Quite good, but a bit too sweet for my taste.\" Rating: 1, Flavor: Mango Tango\n",
        "\"\"\"\n",
        "\n",
        "response = client.models.generate_content(\n",
        "    model=MODEL_ID,\n",
        "    contents=prompt,\n",
        "    config=GenerateContentConfig(\n",
        "        response_mime_type=\"application/json\",\n",
        "        response_schema=response_schema,\n",
        "        thinking_config=thinking_config,\n",
        "    ),\n",
        ")\n",
        "\n",
        "print(response.text)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gV1dR-QlTKRs"
      },
      "source": [
        "## Count tokens and compute tokens\n",
        "\n",
        "You can use the `count_tokens()` method to calculate the number of input tokens before sending a request to the Gemini API.\n",
        "\n",
        "For more information, refer to [list and count tokens](https://cloud.google.com/vertex-ai/generative-ai/docs/multimodal/list-token)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Syx-fwLkV1j-"
      },
      "source": [
        "### Count tokens"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 48,
      "metadata": {
        "id": "UhNElguLRRNK",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "4407fc79-ce75-4c6d-d715-33ac04a6286f"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "sdk_http_response=HttpResponse(\n",
            "  headers=<dict len=9>\n",
            ") total_tokens=9 cached_content_token_count=None\n"
          ]
        }
      ],
      "source": [
        "response = client.models.count_tokens(\n",
        "    model=MODEL_ID,\n",
        "    contents=\"What's the highest mountain in Africa?\",\n",
        ")\n",
        "\n",
        "print(response)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_BsP0vXOY7hg"
      },
      "source": [
        "## Search as a tool (Grounding)\n",
        "\n",
        "[Grounding](https://cloud.google.com/vertex-ai/generative-ai/docs/multimodal/ground-gemini) lets you connect real-world data to the Gemini model.\n",
        "\n",
        "By grounding model responses in Google Search results, the model can access information at runtime that goes beyond its training data which can produce more accurate, up-to-date, and relevant responses.\n",
        "\n",
        "Using Grounding with Google Search, you can improve the accuracy and recency of responses from the model. Starting with Gemini 2.0, Google Search is available as a tool. This means that the model can decide when to use Google Search.\n",
        "\n",
        "For more examples of Grounding, refer to [this notebook](https://github.com/GoogleCloudPlatform/generative-ai/blob/main/gemini/grounding/intro-grounding-gemini.ipynb)."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4_M_4RRBdO_3"
      },
      "source": [
        "### Google Search\n",
        "\n",
        "You can add the `tools` keyword argument with a `Tool` including `GoogleSearch` to instruct Gemini to first perform a Google Search with the prompt, then construct an answer based on the web search results."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 49,
      "metadata": {
        "id": "yeR09J3AZT4U",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "908b4531-c20c-4121-d39a-4debf97ddbd0"
      },
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Markdown object>"
            ],
            "text/markdown": "The current temperature in Austin, TX is 79°F (26°C). The \"feels like\" temperature is 85°F (29°C). The current conditions are partly cloudy with an 84% humidity level."
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "google_maps_widget_context_token=None grounding_chunks=[GroundingChunk(\n",
            "  web=GroundingChunkWeb(\n",
            "    domain='google.com',\n",
            "    title='Weather information for Austin, TX, US',\n",
            "    uri='https://www.google.com/search?q=weather+in+Austin, TX,+US'\n",
            "  )\n",
            ")] grounding_supports=[GroundingSupport(\n",
            "  grounding_chunk_indices=[\n",
            "    0,\n",
            "  ],\n",
            "  segment=Segment(\n",
            "    end_index=55,\n",
            "    text='The current temperature in Austin, TX is 79°F (26°C).'\n",
            "  )\n",
            "), GroundingSupport(\n",
            "  grounding_chunk_indices=[\n",
            "    0,\n",
            "  ],\n",
            "  segment=Segment(\n",
            "    end_index=102,\n",
            "    start_index=56,\n",
            "    text='The \"feels like\" temperature is 85°F (29°C).'\n",
            "  )\n",
            "), GroundingSupport(\n",
            "  grounding_chunk_indices=[\n",
            "    0,\n",
            "  ],\n",
            "  segment=Segment(\n",
            "    end_index=171,\n",
            "    start_index=103,\n",
            "    text='The current conditions are partly cloudy with an 84% humidity level.'\n",
            "  )\n",
            ")] retrieval_metadata=RetrievalMetadata() retrieval_queries=None search_entry_point=SearchEntryPoint(\n",
            "  rendered_content=\"\"\"<style>\n",
            ".container {\n",
            "  align-items: center;\n",
            "  border-radius: 8px;\n",
            "  display: flex;\n",
            "  font-family: Google Sans, Roboto, sans-serif;\n",
            "  font-size: 14px;\n",
            "  line-height: 20px;\n",
            "  padding: 8px 12px;\n",
            "}\n",
            ".chip {\n",
            "  display: inline-block;\n",
            "  border: solid 1px;\n",
            "  border-radius: 16px;\n",
            "  min-width: 14px;\n",
            "  padding: 5px 16px;\n",
            "  text-align: center;\n",
            "  user-select: none;\n",
            "  margin: 0 8px;\n",
            "  -webkit-tap-highlight-color: transparent;\n",
            "}\n",
            ".carousel {\n",
            "  overflow: auto;\n",
            "  scrollbar-width: none;\n",
            "  white-space: nowrap;\n",
            "  margin-right: -12px;\n",
            "}\n",
            ".headline {\n",
            "  display: flex;\n",
            "  margin-right: 4px;\n",
            "}\n",
            ".gradient-container {\n",
            "  position: relative;\n",
            "}\n",
            ".gradient {\n",
            "  position: absolute;\n",
            "  transform: translate(3px, -9px);\n",
            "  height: 36px;\n",
            "  width: 9px;\n",
            "}\n",
            "@media (prefers-color-scheme: light) {\n",
            "  .container {\n",
            "    background-color: #fafafa;\n",
            "    box-shadow: 0 0 0 1px #0000000f;\n",
            "  }\n",
            "  .headline-label {\n",
            "    color: #1f1f1f;\n",
            "  }\n",
            "  .chip {\n",
            "    background-color: #ffffff;\n",
            "    border-color: #d2d2d2;\n",
            "    color: #5e5e5e;\n",
            "    text-decoration: none;\n",
            "  }\n",
            "  .chip:hover {\n",
            "    background-color: #f2f2f2;\n",
            "  }\n",
            "  .chip:focus {\n",
            "    background-color: #f2f2f2;\n",
            "  }\n",
            "  .chip:active {\n",
            "    background-color: #d8d8d8;\n",
            "    border-color: #b6b6b6;\n",
            "  }\n",
            "  .logo-dark {\n",
            "    display: none;\n",
            "  }\n",
            "  .gradient {\n",
            "    background: linear-gradient(90deg, #fafafa 15%, #fafafa00 100%);\n",
            "  }\n",
            "}\n",
            "@media (prefers-color-scheme: dark) {\n",
            "  .container {\n",
            "    background-color: #1f1f1f;\n",
            "    box-shadow: 0 0 0 1px #ffffff26;\n",
            "  }\n",
            "  .headline-label {\n",
            "    color: #fff;\n",
            "  }\n",
            "  .chip {\n",
            "    background-color: #2c2c2c;\n",
            "    border-color: #3c4043;\n",
            "    color: #fff;\n",
            "    text-decoration: none;\n",
            "  }\n",
            "  .chip:hover {\n",
            "    background-color: #353536;\n",
            "  }\n",
            "  .chip:focus {\n",
            "    background-color: #353536;\n",
            "  }\n",
            "  .chip:active {\n",
            "    background-color: #464849;\n",
            "    border-color: #53575b;\n",
            "  }\n",
            "  .logo-light {\n",
            "    display: none;\n",
            "  }\n",
            "  .gradient {\n",
            "    background: linear-gradient(90deg, #1f1f1f 15%, #1f1f1f00 100%);\n",
            "  }\n",
            "}\n",
            "</style>\n",
            "<div class=\"container\">\n",
            "  <div class=\"headline\">\n",
            "    <svg class=\"logo-light\" width=\"18\" height=\"18\" viewBox=\"9 9 35 35\" fill=\"none\" xmlns=\"http://www.w3.org/2000/svg\">\n",
            "      <path fill-rule=\"evenodd\" clip-rule=\"evenodd\" d=\"M42.8622 27.0064C42.8622 25.7839 42.7525 24.6084 42.5487 23.4799H26.3109V30.1568H35.5897C35.1821 32.3041 33.9596 34.1222 32.1258 35.3448V39.6864H37.7213C40.9814 36.677 42.8622 32.2571 42.8622 27.0064V27.0064Z\" fill=\"#4285F4\"/>\n",
            "      <path fill-rule=\"evenodd\" clip-rule=\"evenodd\" d=\"M26.3109 43.8555C30.9659 43.8555 34.8687 42.3195 37.7213 39.6863L32.1258 35.3447C30.5898 36.3792 28.6306 37.0061 26.3109 37.0061C21.8282 37.0061 18.0195 33.9811 16.6559 29.906H10.9194V34.3573C13.7563 39.9841 19.5712 43.8555 26.3109 43.8555V43.8555Z\" fill=\"#34A853\"/>\n",
            "      <path fill-rule=\"evenodd\" clip-rule=\"evenodd\" d=\"M16.6559 29.8904C16.3111 28.8559 16.1074 27.7588 16.1074 26.6146C16.1074 25.4704 16.3111 24.3733 16.6559 23.3388V18.8875H10.9194C9.74388 21.2072 9.06992 23.8247 9.06992 26.6146C9.06992 29.4045 9.74388 32.022 10.9194 34.3417L15.3864 30.8621L16.6559 29.8904V29.8904Z\" fill=\"#FBBC05\"/>\n",
            "      <path fill-rule=\"evenodd\" clip-rule=\"evenodd\" d=\"M26.3109 16.2386C28.85 16.2386 31.107 17.1164 32.9095 18.8091L37.8466 13.8719C34.853 11.082 30.9659 9.3736 26.3109 9.3736C19.5712 9.3736 13.7563 13.245 10.9194 18.8875L16.6559 23.3388C18.0195 19.2636 21.8282 16.2386 26.3109 16.2386V16.2386Z\" fill=\"#EA4335\"/>\n",
            "    </svg>\n",
            "    <svg class=\"logo-dark\" width=\"18\" height=\"18\" viewBox=\"0 0 48 48\" xmlns=\"http://www.w3.org/2000/svg\">\n",
            "      <circle cx=\"24\" cy=\"23\" fill=\"#FFF\" r=\"22\"/>\n",
            "      <path d=\"M33.76 34.26c2.75-2.56 4.49-6.37 4.49-11.26 0-.89-.08-1.84-.29-3H24.01v5.99h8.03c-.4 2.02-1.5 3.56-3.07 4.56v.75l3.91 2.97h.88z\" fill=\"#4285F4\"/>\n",
            "      <path d=\"M15.58 25.77A8.845 8.845 0 0 0 24 31.86c1.92 0 3.62-.46 4.97-1.31l4.79 3.71C31.14 36.7 27.65 38 24 38c-5.93 0-11.01-3.4-13.45-8.36l.17-1.01 4.06-2.85h.8z\" fill=\"#34A853\"/>\n",
            "      <path d=\"M15.59 20.21a8.864 8.864 0 0 0 0 5.58l-5.03 3.86c-.98-2-1.53-4.25-1.53-6.64 0-2.39.55-4.64 1.53-6.64l1-.22 3.81 2.98.22 1.08z\" fill=\"#FBBC05\"/>\n",
            "      <path d=\"M24 14.14c2.11 0 4.02.75 5.52 1.98l4.36-4.36C31.22 9.43 27.81 8 24 8c-5.93 0-11.01 3.4-13.45 8.36l5.03 3.85A8.86 8.86 0 0 1 24 14.14z\" fill=\"#EA4335\"/>\n",
            "    </svg>\n",
            "    <div class=\"gradient-container\"><div class=\"gradient\"></div></div>\n",
            "  </div>\n",
            "  <div class=\"carousel\">\n",
            "    <a class=\"chip\" href=\"https://vertexaisearch.cloud.google.com/grounding-api-redirect/AUZIYQE8S9NvpoXuIRDN4dyAJfX9WvZgzs3bx4VakKwMrJl62RFwo2VuSVR-hIouDUecwj64d2fl0E_GeFuejXQ3XG4F6lTySjovP7Qq2ujh2Y9Ps1x4oys4nkorV3dUyRpTGxxSo2r6TpazPXfbO50EYULCHaXsPk-QUbCYrKRyQGeNu__kJ1SLHBSum3egHG-cY5xgWq5eRaV8TPBZZrYWj3e3Gs91vg==\">current temperature in Austin, TX</a>\n",
            "  </div>\n",
            "</div>\n",
            "\"\"\"\n",
            ") web_search_queries=['current temperature in Austin, TX']\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "<style>\n",
              ".container {\n",
              "  align-items: center;\n",
              "  border-radius: 8px;\n",
              "  display: flex;\n",
              "  font-family: Google Sans, Roboto, sans-serif;\n",
              "  font-size: 14px;\n",
              "  line-height: 20px;\n",
              "  padding: 8px 12px;\n",
              "}\n",
              ".chip {\n",
              "  display: inline-block;\n",
              "  border: solid 1px;\n",
              "  border-radius: 16px;\n",
              "  min-width: 14px;\n",
              "  padding: 5px 16px;\n",
              "  text-align: center;\n",
              "  user-select: none;\n",
              "  margin: 0 8px;\n",
              "  -webkit-tap-highlight-color: transparent;\n",
              "}\n",
              ".carousel {\n",
              "  overflow: auto;\n",
              "  scrollbar-width: none;\n",
              "  white-space: nowrap;\n",
              "  margin-right: -12px;\n",
              "}\n",
              ".headline {\n",
              "  display: flex;\n",
              "  margin-right: 4px;\n",
              "}\n",
              ".gradient-container {\n",
              "  position: relative;\n",
              "}\n",
              ".gradient {\n",
              "  position: absolute;\n",
              "  transform: translate(3px, -9px);\n",
              "  height: 36px;\n",
              "  width: 9px;\n",
              "}\n",
              "@media (prefers-color-scheme: light) {\n",
              "  .container {\n",
              "    background-color: #fafafa;\n",
              "    box-shadow: 0 0 0 1px #0000000f;\n",
              "  }\n",
              "  .headline-label {\n",
              "    color: #1f1f1f;\n",
              "  }\n",
              "  .chip {\n",
              "    background-color: #ffffff;\n",
              "    border-color: #d2d2d2;\n",
              "    color: #5e5e5e;\n",
              "    text-decoration: none;\n",
              "  }\n",
              "  .chip:hover {\n",
              "    background-color: #f2f2f2;\n",
              "  }\n",
              "  .chip:focus {\n",
              "    background-color: #f2f2f2;\n",
              "  }\n",
              "  .chip:active {\n",
              "    background-color: #d8d8d8;\n",
              "    border-color: #b6b6b6;\n",
              "  }\n",
              "  .logo-dark {\n",
              "    display: none;\n",
              "  }\n",
              "  .gradient {\n",
              "    background: linear-gradient(90deg, #fafafa 15%, #fafafa00 100%);\n",
              "  }\n",
              "}\n",
              "@media (prefers-color-scheme: dark) {\n",
              "  .container {\n",
              "    background-color: #1f1f1f;\n",
              "    box-shadow: 0 0 0 1px #ffffff26;\n",
              "  }\n",
              "  .headline-label {\n",
              "    color: #fff;\n",
              "  }\n",
              "  .chip {\n",
              "    background-color: #2c2c2c;\n",
              "    border-color: #3c4043;\n",
              "    color: #fff;\n",
              "    text-decoration: none;\n",
              "  }\n",
              "  .chip:hover {\n",
              "    background-color: #353536;\n",
              "  }\n",
              "  .chip:focus {\n",
              "    background-color: #353536;\n",
              "  }\n",
              "  .chip:active {\n",
              "    background-color: #464849;\n",
              "    border-color: #53575b;\n",
              "  }\n",
              "  .logo-light {\n",
              "    display: none;\n",
              "  }\n",
              "  .gradient {\n",
              "    background: linear-gradient(90deg, #1f1f1f 15%, #1f1f1f00 100%);\n",
              "  }\n",
              "}\n",
              "</style>\n",
              "<div class=\"container\">\n",
              "  <div class=\"headline\">\n",
              "    <svg class=\"logo-light\" width=\"18\" height=\"18\" viewBox=\"9 9 35 35\" fill=\"none\" xmlns=\"http://www.w3.org/2000/svg\">\n",
              "      <path fill-rule=\"evenodd\" clip-rule=\"evenodd\" d=\"M42.8622 27.0064C42.8622 25.7839 42.7525 24.6084 42.5487 23.4799H26.3109V30.1568H35.5897C35.1821 32.3041 33.9596 34.1222 32.1258 35.3448V39.6864H37.7213C40.9814 36.677 42.8622 32.2571 42.8622 27.0064V27.0064Z\" fill=\"#4285F4\"/>\n",
              "      <path fill-rule=\"evenodd\" clip-rule=\"evenodd\" d=\"M26.3109 43.8555C30.9659 43.8555 34.8687 42.3195 37.7213 39.6863L32.1258 35.3447C30.5898 36.3792 28.6306 37.0061 26.3109 37.0061C21.8282 37.0061 18.0195 33.9811 16.6559 29.906H10.9194V34.3573C13.7563 39.9841 19.5712 43.8555 26.3109 43.8555V43.8555Z\" fill=\"#34A853\"/>\n",
              "      <path fill-rule=\"evenodd\" clip-rule=\"evenodd\" d=\"M16.6559 29.8904C16.3111 28.8559 16.1074 27.7588 16.1074 26.6146C16.1074 25.4704 16.3111 24.3733 16.6559 23.3388V18.8875H10.9194C9.74388 21.2072 9.06992 23.8247 9.06992 26.6146C9.06992 29.4045 9.74388 32.022 10.9194 34.3417L15.3864 30.8621L16.6559 29.8904V29.8904Z\" fill=\"#FBBC05\"/>\n",
              "      <path fill-rule=\"evenodd\" clip-rule=\"evenodd\" d=\"M26.3109 16.2386C28.85 16.2386 31.107 17.1164 32.9095 18.8091L37.8466 13.8719C34.853 11.082 30.9659 9.3736 26.3109 9.3736C19.5712 9.3736 13.7563 13.245 10.9194 18.8875L16.6559 23.3388C18.0195 19.2636 21.8282 16.2386 26.3109 16.2386V16.2386Z\" fill=\"#EA4335\"/>\n",
              "    </svg>\n",
              "    <svg class=\"logo-dark\" width=\"18\" height=\"18\" viewBox=\"0 0 48 48\" xmlns=\"http://www.w3.org/2000/svg\">\n",
              "      <circle cx=\"24\" cy=\"23\" fill=\"#FFF\" r=\"22\"/>\n",
              "      <path d=\"M33.76 34.26c2.75-2.56 4.49-6.37 4.49-11.26 0-.89-.08-1.84-.29-3H24.01v5.99h8.03c-.4 2.02-1.5 3.56-3.07 4.56v.75l3.91 2.97h.88z\" fill=\"#4285F4\"/>\n",
              "      <path d=\"M15.58 25.77A8.845 8.845 0 0 0 24 31.86c1.92 0 3.62-.46 4.97-1.31l4.79 3.71C31.14 36.7 27.65 38 24 38c-5.93 0-11.01-3.4-13.45-8.36l.17-1.01 4.06-2.85h.8z\" fill=\"#34A853\"/>\n",
              "      <path d=\"M15.59 20.21a8.864 8.864 0 0 0 0 5.58l-5.03 3.86c-.98-2-1.53-4.25-1.53-6.64 0-2.39.55-4.64 1.53-6.64l1-.22 3.81 2.98.22 1.08z\" fill=\"#FBBC05\"/>\n",
              "      <path d=\"M24 14.14c2.11 0 4.02.75 5.52 1.98l4.36-4.36C31.22 9.43 27.81 8 24 8c-5.93 0-11.01 3.4-13.45 8.36l5.03 3.85A8.86 8.86 0 0 1 24 14.14z\" fill=\"#EA4335\"/>\n",
              "    </svg>\n",
              "    <div class=\"gradient-container\"><div class=\"gradient\"></div></div>\n",
              "  </div>\n",
              "  <div class=\"carousel\">\n",
              "    <a class=\"chip\" href=\"https://vertexaisearch.cloud.google.com/grounding-api-redirect/AUZIYQE8S9NvpoXuIRDN4dyAJfX9WvZgzs3bx4VakKwMrJl62RFwo2VuSVR-hIouDUecwj64d2fl0E_GeFuejXQ3XG4F6lTySjovP7Qq2ujh2Y9Ps1x4oys4nkorV3dUyRpTGxxSo2r6TpazPXfbO50EYULCHaXsPk-QUbCYrKRyQGeNu__kJ1SLHBSum3egHG-cY5xgWq5eRaV8TPBZZrYWj3e3Gs91vg==\">current temperature in Austin, TX</a>\n",
              "  </div>\n",
              "</div>\n"
            ]
          },
          "metadata": {},
          "execution_count": 49
        }
      ],
      "source": [
        "google_search_tool = Tool(google_search=GoogleSearch())\n",
        "\n",
        "response = client.models.generate_content(\n",
        "    model=MODEL_ID,\n",
        "    contents=\"What is the current temperature in Austin, TX?\",\n",
        "    config=GenerateContentConfig(\n",
        "        tools=[google_search_tool],\n",
        "        thinking_config=thinking_config,\n",
        "    ),\n",
        ")\n",
        "\n",
        "display(Markdown(response.text))\n",
        "\n",
        "print(response.candidates[0].grounding_metadata)\n",
        "\n",
        "HTML(response.candidates[0].grounding_metadata.search_entry_point.rendered_content)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "T0pb-Kh1xEHU"
      },
      "source": [
        "## Function calling\n",
        "\n",
        "[Function Calling](https://cloud.google.com/vertex-ai/docs/generative-ai/multimodal/function-calling) in Gemini lets developers create a description of a function in their code, then pass that description to a language model in a request.\n",
        "\n",
        "You can submit a Python function for automatic function calling, which will run the function and return the output in natural language generated by Gemini.\n",
        "\n",
        "You can also submit an [OpenAPI Specification](https://www.openapis.org/) which will respond with the name of a function that matches the description and the arguments to call it with.\n",
        "\n",
        "For more examples of Function calling with Gemini, check out this notebook: [Intro to Function Calling with Gemini](https://github.com/GoogleCloudPlatform/generative-ai/blob/main/gemini/function-calling/intro_function_calling.ipynb)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mSUWWlrrlR-D"
      },
      "source": [
        "### Python Function (Automatic Function Calling)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 50,
      "metadata": {
        "id": "aRR8HZhLlR-E",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 46
        },
        "outputId": "859815fe-b5ad-4123-ed4c-a3ae30fa58cf"
      },
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Markdown object>"
            ],
            "text/markdown": "The weather in San Francisco is foggy.\n"
          },
          "metadata": {}
        }
      ],
      "source": [
        "def get_current_weather(location: str) -> str:\n",
        "    \"\"\"Example method. Returns the current weather.\n",
        "\n",
        "    Args:\n",
        "        location: The city and state, e.g. San Francisco, CA\n",
        "    \"\"\"\n",
        "    weather_map: dict[str, str] = {\n",
        "        \"Boston, MA\": \"snowing\",\n",
        "        \"San Francisco, CA\": \"foggy\",\n",
        "        \"Seattle, WA\": \"raining\",\n",
        "        \"Austin, TX\": \"hot\",\n",
        "        \"Chicago, IL\": \"windy\",\n",
        "    }\n",
        "    return weather_map.get(location, \"unknown\")\n",
        "\n",
        "\n",
        "response = client.models.generate_content(\n",
        "    model=MODEL_ID,\n",
        "    contents=\"What is the weather like in San Francisco?\",\n",
        "    config=GenerateContentConfig(\n",
        "        tools=[get_current_weather],\n",
        "        temperature=0,\n",
        "        thinking_config=thinking_config,\n",
        "    ),\n",
        ")\n",
        "\n",
        "display(Markdown(response.text))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "h4syyLEClGcn"
      },
      "source": [
        "### OpenAPI Specification (Manual Function Calling)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 51,
      "metadata": {
        "id": "2BDQPwgcxRN3",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "60a9800c-7004-4746-ee82-117b426baa7f"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "id=None args={'destination': 'Paris'} name='get_destination'\n"
          ]
        }
      ],
      "source": [
        "get_destination = FunctionDeclaration(\n",
        "    name=\"get_destination\",\n",
        "    description=\"Get the destination that the user wants to go to\",\n",
        "    parameters={\n",
        "        \"type\": \"OBJECT\",\n",
        "        \"properties\": {\n",
        "            \"destination\": {\n",
        "                \"type\": \"STRING\",\n",
        "                \"description\": \"Destination that the user wants to go to\",\n",
        "            },\n",
        "        },\n",
        "    },\n",
        ")\n",
        "\n",
        "destination_tool = Tool(\n",
        "    function_declarations=[get_destination],\n",
        ")\n",
        "\n",
        "response = client.models.generate_content(\n",
        "    model=MODEL_ID,\n",
        "    contents=\"I'd like to travel to Paris.\",\n",
        "    config=GenerateContentConfig(\n",
        "        tools=[destination_tool],\n",
        "        temperature=0,\n",
        "        thinking_config=thinking_config,\n",
        "    ),\n",
        ")\n",
        "\n",
        "print(response.function_calls[0])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MhDs2X3o0neK"
      },
      "source": [
        "## Code Execution\n",
        "\n",
        "The Gemini API [code execution](https://ai.google.dev/gemini-api/docs/code-execution?lang=python) feature enables the model to generate and run Python code and learn iteratively from the results until it arrives at a final output. You can use this code execution capability to build applications that benefit from code-based reasoning and that produce text output. For example, you could use code execution in an application that solves equations or processes text.\n",
        "\n",
        "The Gemini API provides code execution as a tool, similar to function calling.\n",
        "After you add code execution as a tool, the model decides when to use it.\n",
        "\n",
        "For more examples of Code Execution, refer to [this notebook](https://github.com/GoogleCloudPlatform/generative-ai/blob/main/gemini/code-execution/intro_code_execution.ipynb)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 52,
      "metadata": {
        "id": "1W-3c7sy0nyz",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 348
        },
        "outputId": "cd56c0c3-dab1-470f-9085-076bbde001d6"
      },
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Markdown object>"
            ],
            "text/markdown": "\n## Code\n\n```py\ndef fibonacci(n):\n    \"\"\"Calculates the nth Fibonacci number.\"\"\"\n    if n <= 0:\n        return 0\n    elif n == 1:\n        return 1\n    a, b = 0, 1\n    for _ in range(2, n + 1):\n        a, b = b, a + b\n    return b\n\nfib_20 = fibonacci(20)\nprint(f\"The 20th Fibonacci number is: {fib_20}\")\n\n```\n\n### Output\n\n```\nThe 20th Fibonacci number is: 6765\n\n```\n"
          },
          "metadata": {}
        }
      ],
      "source": [
        "code_execution_tool = Tool(code_execution=ToolCodeExecution())\n",
        "\n",
        "response = client.models.generate_content(\n",
        "    model=MODEL_ID,\n",
        "    contents=\"Calculate 20th fibonacci number. Then find the nearest palindrome to it.\",\n",
        "    config=GenerateContentConfig(\n",
        "        tools=[code_execution_tool],\n",
        "        temperature=0,\n",
        "        thinking_config=thinking_config,\n",
        "    ),\n",
        ")\n",
        "\n",
        "display(\n",
        "    Markdown(\n",
        "        f\"\"\"\n",
        "## Code\n",
        "\n",
        "```py\n",
        "{response.executable_code}\n",
        "```\n",
        "\n",
        "### Output\n",
        "\n",
        "```\n",
        "{response.code_execution_result}\n",
        "```\n",
        "\"\"\"\n",
        "    )\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eQwiONFdVHw5"
      },
      "source": [
        "## What's next\n",
        "\n",
        "- See the [Google Gen AI SDK reference docs](https://googleapis.github.io/python-genai/).\n",
        "- Explore other notebooks in the [Google Cloud Generative AI GitHub repository](https://github.com/GoogleCloudPlatform/generative-ai).\n",
        "- Explore AI models in [Model Garden](https://cloud.google.com/vertex-ai/generative-ai/docs/model-garden/explore-models)."
      ]
    }
  ],
  "metadata": {
    "colab": {
      "name": "intro_gemini_2_5_pro.ipynb",
      "toc_visible": true,
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}